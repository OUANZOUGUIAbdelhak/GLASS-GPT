{
  "id": "0130294b-d3dd-4db9-84ec-bd9043e6bc1d",
  "data": {
    "nodes": [
      {
        "id": "ParseData-O3xbr",
        "type": "genericNode",
        "position": {
          "x": 1108.481456822171,
          "y": 3500.80325057382
        },
        "data": {
          "description": "Convert Data into plain text following a specified template.",
          "display_name": "Parse Data",
          "id": "ParseData-O3xbr",
          "node": {
            "template": {
              "_type": "Component",
              "data": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "trace_as_input": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "data",
                "value": "",
                "display_name": "Data",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "The data to convert to text.",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "sep": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sep",
                "value": "\n",
                "display_name": "Separator",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "{text}",
                "display_name": "Template",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Convert Data into plain text following a specified template.",
            "icon": "braces",
            "base_classes": [
              "Message"
            ],
            "display_name": "Parse Data",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "hidden": null,
                "display_name": "Text",
                "method": "parse_data",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "data",
              "template",
              "sep"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "ParseData"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 295
        }
      },
      {
        "id": "Prompt-CvBi7",
        "type": "genericNode",
        "position": {
          "x": 3860.9657169156617,
          "y": 1191.268393370288
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-CvBi7",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 3 and 4 as listed in the user’s input (verre_3 and verre_4). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 4 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 2 glasses are present, glasses 3 and 4 must have all values set to [0]; if only 3 glasses are present, glass 4 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 3:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 4:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 3 and 4, based on their order in the document (verre_3 is the 3rd glass, verre_4 is the 4th glass).  \n- If the article contains fewer than 4 glasses (e.g., only 2 or 3), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 4 (if present). For glasses 3 and 4 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- Headers must be exactly \"Glass 3:\" and \"Glass 4:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 3 and Glass 4, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 3 and Glass 4), even if the article has fewer than 3 or 4 glasses.\n",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G3-4",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-uYxja",
        "type": "genericNode",
        "position": {
          "x": 1851.2170313537376,
          "y": 2816.1974049127125
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "Répondre à ce qui est demandé dans le prompt ",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 10000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "learnlm-2.0-flash-experimental",
                  "gemma-3n-e4b-it",
                  "gemma-3n-e2b-it",
                  "gemma-3-4b-it",
                  "gemma-3-27b-it",
                  "gemma-3-1b-it",
                  "gemma-3-12b-it",
                  "gemini-pro-vision",
                  "gemini-exp-1206",
                  "gemini-2.5-pro-preview-tts",
                  "gemini-2.5-pro-preview-06-05",
                  "gemini-2.5-pro-preview-05-06",
                  "gemini-2.5-pro-preview-03-25",
                  "gemini-2.5-pro",
                  "gemini-2.5-flash-preview-tts",
                  "gemini-2.5-flash-preview-05-20",
                  "gemini-2.5-flash-preview-04-17-thinking",
                  "gemini-2.5-flash-preview-04-17",
                  "gemini-2.5-flash-lite-preview-06-17",
                  "gemini-2.5-flash",
                  "gemini-2.0-pro-exp-02-05",
                  "gemini-2.0-pro-exp",
                  "gemini-2.0-flash-thinking-exp-1219",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "gemini-2.0-flash-thinking-exp",
                  "gemini-2.0-flash-lite-preview-02-05",
                  "gemini-2.0-flash-lite-preview",
                  "gemini-2.0-flash-lite-001",
                  "gemini-2.0-flash-lite",
                  "gemini-2.0-flash-exp",
                  "gemini-2.0-flash-001",
                  "gemini-2.0-flash",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-pro",
                  "gemini-1.5-flash-latest",
                  "gemini-1.5-flash-8b-latest",
                  "gemini-1.5-flash-8b-001",
                  "gemini-1.5-flash-8b",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-flash",
                  "gemini-1.0-pro-vision-latest"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-uYxja"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-Prpyc",
        "type": "genericNode",
        "position": {
          "x": 1468.0731847457673,
          "y": 3019.38390402413
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code"
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "Veuillez lire le document ci-dessus et extraire uniquement les informations suivantes. Formatez les réponses clairement pour chaque élément :\n\n**Document :**\n{Document}\n\n---\n\n**Informations demandées :**\nExtraire uniquement les noms officiels des verres dont la composition est explicitement donnée dans le document et pour lesquels au moins un paramètre expérimental est disponible. Ne pas extraire les verres simplement mentionnés sans composition ou paramètres expérimentaux, ni les noms secondaires ou alternatifs qui n'ont pas de caractéristiques numériques dans le document.\n\n1. verre_1 : [Nom clair, sans texte additionnel]\n2. verre_2 : [Nom clair, sans texte additionnel]\n3. verre_3 : [Nom clair, sans texte additionnel]\n4. verre_4 : [Nom clair, sans texte additionnel]\n...\n\n---\n\n**Format attendu :**\n\n1. verre_1 : [Nom]\n2. verre_2 : [Nom]\n3. verre_3 : [Nom]\n4. verre_4 : [Nom]\n...\nn. verre_n : [Nom]\n\n---\n\n**Instructions strictes :**\n- N'extraire que les verres dont la composition et au moins un paramètre expérimental sont explicitement mentionnés dans le document.\n- Ne pas inclure de noms secondaires ou alternatifs sans caractéristiques numériques.\n- Ne pas ajouter de commentaires supplémentaires et respecter strictement le format attendu de la réponse.\n- Si un verre ne répond pas aux critères, ne pas l'inclure dans la liste.\n- Si un verre a une composition théorique et une composition expérimentale, il faut prendre celui dont on a la composition expérimentale. j'insiste beaucoup sur la composition expérimentale. ",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-Prpyc"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "Prompt-95tm9",
        "type": "genericNode",
        "position": {
          "x": 3833.0443976026586,
          "y": 196.9635425350322
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the list of glass type names in ascending order provided by the user in the input. Extract the following information and format the responses clearly for each item. Ensure that the extraction handles variations such as spaces after colons (\":\").\n\nDocument:\n{Document}\n\n---\n\nInformation requested:\n\n1. Type du document : [Type]\n2. Titre du document : [Titre]\n3. Référence : [Référence]\n4. Premier Auteur : [Full Name]\n5. Nombre de types de verres : [Value, Total number of glass types mentioned in the list provided by the user. Count the number of glass types present in this list.]\n\n---\n\nExpected format:\n\n1. Type du document : [Type]\n2. Titre du document : [Titre]\n3. Référence : [Référence]\n4. Premier Auteur : [Premier Auteur]\n5. Nombre de types de verres : [Value]\n\n---\n\nIf any information is unavailable, indicate \"Non disponible\". For the number of glass types, count the number of glass types present in the list provided by the user in the input. Do not add asterisks (**) to make the text bold. Do not add any comments at the beginning or end.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt2",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-95tm9"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "Prompt-BTfkR",
        "type": "genericNode",
        "position": {
          "x": 3852.7534636300516,
          "y": 721.7226201214179
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 1 and 2 as listed in the user’s input (verre_1 and verre_2). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 2 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 1 glass is present, glass 2 must have all values set to [0]; if no glasses are present, both glasses 1 and 2 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 1:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 2:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 1 and 2, based on their order in the document (verre_1 is the 1st glass, verre_2 is the 2nd glass).  \n- If the article contains fewer than 2 glasses (e.g., only 1), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the missing glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 and 2 (if present). For glass 2 not present, replicate the same list of oxides/elements from glass 1 (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.  \n- Headers must be exactly \"Glass 1:\" and \"Glass 2:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 1 and Glass 2, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 1 and Glass 2), even if the article has fewer than 1 or 2 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G1-2",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-BTfkR"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "GlassCompositionConverter-nDAxX",
        "type": "genericNode",
        "position": {
          "x": 4637.952307432721,
          "y": 809.4553316940747
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-nDAxX"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        },
        "dragging": false
      },
      {
        "id": "CombineText-2gWvC",
        "type": "genericNode",
        "position": {
          "x": 5027.059165021538,
          "y": 734.9860713264267
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-2gWvC"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        },
        "dragging": false
      },
      {
        "id": "Prompt-KHumU",
        "type": "genericNode",
        "position": {
          "x": 5458.1382275757605,
          "y": 737.9218313672599
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the first two glass types listed in the input (1. verre_1 :, 2. verre_2 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n...  \nSearch only for the first four glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the first two glass types in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n6. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n7. **Nombre de tests** : Number of tests for this glass type\n8. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n9. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n10. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n11. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n12. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n13. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n14. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n15. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n16. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n17. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n18. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n19. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n20. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n21. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n22. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n23. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n24. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n25. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n26. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n27. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n28. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n29. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n30. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n31. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n32. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n33. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n34. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n35. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n36. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n37. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n38. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n39. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n40. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n41. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n42. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n43. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n44. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n45. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n46. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n47. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n48. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n49. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n50. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n51. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n52. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n53. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n54. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n55. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n56. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n57. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n58. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n59. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n60. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n61. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n62. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n63. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n64. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n65. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n66. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n67. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n68. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n69. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n70. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n71. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n72. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n73. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n74. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n75. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n76. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n77. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n78. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n79. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n80. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n81. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n82. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n83. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n84. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n85. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n86. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n87. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n88. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n89. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n90. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n91. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n92. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n93. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n94. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n95. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n96. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60.] \n97. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n98. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n99. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n100. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n101. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n102. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n103. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n104. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n105. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n106. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n107. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n108. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n109. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type1 to Verre_type2), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n6. Verre_type1 : Type1\n7. Nombre de tests(Verre_type1) : Number\n8. Li(Verre_type1) : [value1, value2, ...]  \n9. B(Verre_type1) : [value1, value2, ...]  \n10. O(Verre_type1) : [value1, value2, ...]  \n...  \n109. Congruence(Verre_type1) : [comment1, comment2, ...]  \n\n110. Verre_type2 : [Type2]  \n111. Nombre de tests(Verre_type2) : [Number]  \n112. Li(Verre_type2) : [value1, value2, ...]  \n...  \n213. Congruence(Verre_type2) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the first two glass types from 6 to 213, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than two glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G1-2",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-KHumU"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-FfeYQ",
        "type": "genericNode",
        "position": {
          "x": 5867.337315609171,
          "y": 655.9219565496792
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "learnlm-2.0-flash-experimental",
                  "gemma-3n-e4b-it",
                  "gemma-3n-e2b-it",
                  "gemma-3-4b-it",
                  "gemma-3-27b-it",
                  "gemma-3-1b-it",
                  "gemma-3-12b-it",
                  "gemini-pro-vision",
                  "gemini-exp-1206",
                  "gemini-2.5-pro-preview-tts",
                  "gemini-2.5-pro-preview-06-05",
                  "gemini-2.5-pro-preview-05-06",
                  "gemini-2.5-pro-preview-03-25",
                  "gemini-2.5-pro",
                  "gemini-2.5-flash-preview-tts",
                  "gemini-2.5-flash-preview-05-20",
                  "gemini-2.5-flash-preview-04-17-thinking",
                  "gemini-2.5-flash-preview-04-17",
                  "gemini-2.5-flash-lite-preview-06-17",
                  "gemini-2.5-flash",
                  "gemini-2.0-pro-exp-02-05",
                  "gemini-2.0-pro-exp",
                  "gemini-2.0-flash-thinking-exp-1219",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "gemini-2.0-flash-thinking-exp",
                  "gemini-2.0-flash-lite-preview-02-05",
                  "gemini-2.0-flash-lite-preview",
                  "gemini-2.0-flash-lite-001",
                  "gemini-2.0-flash-lite",
                  "gemini-2.0-flash-exp",
                  "gemini-2.0-flash-001",
                  "gemini-2.0-flash",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-pro",
                  "gemini-1.5-flash-latest",
                  "gemini-1.5-flash-8b-latest",
                  "gemini-1.5-flash-8b-001",
                  "gemini-1.5-flash-8b",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-flash",
                  "gemini-1.0-pro-vision-latest"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.0-flash",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-FfeYQ"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-H9Xox",
        "type": "genericNode",
        "position": {
          "x": 4221.562451111742,
          "y": 1229.5698310373787
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-H9Xox"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        },
        "dragging": false
      },
      {
        "id": "GlassCompositionConverter-Sblyw",
        "type": "genericNode",
        "position": {
          "x": 4593.631517425632,
          "y": 1246.539938261112
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-Sblyw"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-DHXvR",
        "type": "genericNode",
        "position": {
          "x": 5025.69206187705,
          "y": 1198.5638411898128
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-DHXvR"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-Uyk32",
        "type": "genericNode",
        "position": {
          "x": 5460.374269944515,
          "y": 1210.000850925466
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 3 and 4 in the input (3. verre_3 :, 4. verre_4 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n...  \nSearch only for the first four glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 3 and 4 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n214. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n215. **Nombre de tests** : Number of tests for this glass type\n216. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n217. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n218. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n219. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n220. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n221. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n222. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n223. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n224. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n225. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n226. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n227. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n228. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n229. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n230. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n231. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n232. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n233. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n234. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n235. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n236. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n237. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n238. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n239. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n240. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n241. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n242. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n243. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n244. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n245. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n246. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n247. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n248. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n249. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n250. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n251. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n252. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n253. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n254. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n255. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n256. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n257. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n258. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n259. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n260. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n261. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n262. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n263. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n264. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n265. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n266. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n267. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n268. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n269. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n270. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n271. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n272. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n273. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n274. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n275. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n276. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n277. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n278. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n279. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n280. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n281. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n282. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n283. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n284. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n285. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n286. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n287. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n288. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n289. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n290. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n291. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n292. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n293. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n294. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n295. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n296. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n297. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n298. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n299. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n300. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n301. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n302. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n303. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n304. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n305. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n306. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n307. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n308. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n309. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n310. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n311. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n312. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n313. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n314. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n315. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n316. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n317. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n**Expected Format:**\n\nFor each glass type (Verre_type3 to Verre_type4), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n214. Verre_type3 : Type3  \n215. Nombre de tests(Verre_type3) : Number  \n216. Li(Verre_type3) : [value1, value2, ...]  \n217. B(Verre_type3) : [value1, value2, ...]  \n218. O(Verre_type3) : [value1, value2, ...]  \n...  \n317. Congruence(Verre_type3) : [comment1, comment2, ...]  \n\n318. Verre_type4 : Type4  \n319. Nombre de tests(Verre_type4) : Number \n320. Li(Verre_type4) : [value1, value2, ...]  \n...  \n421. Congruence(Verre_type4) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 3 and 4 from 214 to 421, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than four glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G3-4",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-Uyk32"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-yDfoE",
        "type": "genericNode",
        "position": {
          "x": 5871.186569720579,
          "y": 1158.1224233608627
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-yDfoE"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-VOPkO",
        "type": "genericNode",
        "position": {
          "x": 3860.0004263252868,
          "y": 1591.2314758228918
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-VOPkO",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 5 and 6 as listed in the user’s input (verre_5 and verre_6). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 6 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 4 glasses are present, glasses 5 and 6 must have all values set to [0]; if only 5 glasses are present, glass 6 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 5:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 6:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 5 and 6, based on their order in the document (verre_5 is the 5th glass, verre_6 is the 6th glass).  \n- If the article contains fewer than 6 glasses (e.g., only 4 or 5), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 6 (if present). For glasses 5 and 6 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 5:\" and \"Glass 6:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 5 and Glass 6, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 5 and Glass 6), even if the article has fewer than 5 or 6 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G5-6",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GlassCompositionConverter-WHWvv",
        "type": "genericNode",
        "position": {
          "x": 4646.095809662074,
          "y": 1660.0216056988954
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-WHWvv"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-bA6y9",
        "type": "genericNode",
        "position": {
          "x": 5078.156354113493,
          "y": 1612.0455086275967
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-bA6y9"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-po5vR",
        "type": "genericNode",
        "position": {
          "x": 5498.472490668326,
          "y": 1637.8485898758809
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 5 and 6 in the input (5. verre_5 :, 6. verre_6 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n...  \nSearch only for the first six glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 5 and 6 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n422. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n423. **Nombre de tests** : Number of tests for this glass type\n424. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n425. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n426. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n427. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n428. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n429. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n430. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n431. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n432. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n433. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n434. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n435. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n436. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n437. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n438. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n439. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n440. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n441. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n442. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n443. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n444. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n445. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n446. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n447. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n448. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n449. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n450. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n451. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n452. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n453. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n454. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n455. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n456. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n457. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n458. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n459. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n460. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n461. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n462. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n463. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n464. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n465. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n466. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n467. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n468. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n469. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n470. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n471. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n472. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n473. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n474. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n475. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n476. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n477. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n478. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n479. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n480. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n481. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n482. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n483. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n484. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n485. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n486. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n487. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n488. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n489. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n490. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n491. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n492. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n493. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n494. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n495. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n496. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n497. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n498. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n499. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n500. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n501. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n502. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n503. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n504. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n505. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n506. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n507. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n508. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n509. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n510. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n511. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n512. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]  \n513. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n514. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n515. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n516. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n517. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n518. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n519. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n520. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n521. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n522. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n523. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n524. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n525. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type5 to Verre_type6), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n422. Verre_type5 : Type5  \n423. Nombre de tests(Verre_type5) : Number  \n424. Li(Verre_type5) : [value1, value2, ...]  \n425. B(Verre_type5) : [value1, value2, ...]  \n426. O(Verre_type5) : [value1, value2, ...]  \n...  \n525. Congruence(Verre_type5) : [comment1, comment2, ...]  \n\n526. Verre_type6 : Type6  \n527. Nombre de tests(Verre_type6) : Number  \n528. Li(Verre_type6) : [value1, value2, ...]  \n...  \n629. Congruence(Verre_type6) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 5 and 6 from 422 to 629, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than six glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G5-6",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-po5vR"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-ppBan",
        "type": "genericNode",
        "position": {
          "x": 5875.801283960688,
          "y": 1592.3498818067662
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-ppBan"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 488
        }
      },
      {
        "id": "Prompt-LEw60",
        "type": "genericNode",
        "position": {
          "x": 3870.3141691798173,
          "y": 2040.6456831006226
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-LEw60",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 7 and 8 as listed in the user’s input (verre_7 and verre_8). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 8 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 6 glasses are present, glasses 7 and 8 must have all values set to [0]; if only 7 glasses are present, glass 8 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 7:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 8:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 7 and 8, based on their order in the document (verre_7 is the 7th glass, verre_8 is the 8th glass).  \n- If the article contains fewer than 8 glasses (e.g., only 6 or 7), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 8 (if present). For glasses 7 and 8 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 7:\" and \"Glass 8:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 7 and Glass 8, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 7 and Glass 8), even if the article has fewer than 7 or 8 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G7-8",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-VXx57",
        "type": "genericNode",
        "position": {
          "x": 4262.283177636113,
          "y": 1980.9000422119375
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-VXx57"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-IieDF",
        "type": "genericNode",
        "position": {
          "x": 4656.409552516605,
          "y": 2109.435812976626
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-IieDF"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-1leNx",
        "type": "genericNode",
        "position": {
          "x": 5088.4700969680225,
          "y": 2061.4597159053274
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-1leNx"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-Bia8u",
        "type": "genericNode",
        "position": {
          "x": 5523.152305035487,
          "y": 2072.8967256409805
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 7 and 8 in the input (7. verre_7 :, 8. verre_8 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n...  \nSearch only for the first eight glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 7 and 8 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n630. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n631. **Nombre de tests** : Number of tests for this glass type\n632. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n633. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n634. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n635. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n636. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n637. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n638. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n639. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n640. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n641. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n642. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n643. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n644. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n645. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n646. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n647. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n648. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n649. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n650. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n651. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n652. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n653. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n654. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n655. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n656. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n657. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n658. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n659. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n660. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n661. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n662. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n663. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n664. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n665. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n666. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n667. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n668. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n669. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n670. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n671. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n672. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n673. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n674. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n675. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n676. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n677. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n678. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n679. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n680. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n681. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n682. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n683. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n684. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n685. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n686. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n687. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n688. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n689. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n690. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n691. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n692. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n693. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n694. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n695. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n696. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n697. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n698. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n699. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n700. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n701. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n702. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n703. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n704. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n705. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n706. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n707. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n708. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n709. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n710. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n711. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n712. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n713. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n714. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n715. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n716. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n717. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n718. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n719. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n720. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]  \n721. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n722. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n723. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n724. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n725. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n726. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n727. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n728. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n729. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n730. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n731. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n732. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n733. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type7 to Verre_type8), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n630. Verre_type7 : Type7  \n631. Nombre de tests(Verre_type7) : Number  \n632. Li(Verre_type7) : [value1, value2, ...]  \n633. B(Verre_type7) : [value1, value2, ...]  \n634. O(Verre_type7) : [value1, value2, ...]  \n...  \n733. Congruence(Verre_type7) : [comment1, comment2, ...]  \n\n734. Verre_type8 : Type8  \n735. Nombre de tests(Verre_type8) : Number  \n736. Li(Verre_type8) : [value1, value2, ...]  \n...  \n837. Congruence(Verre_type8) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 7 and 8 from 630 to 837, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than eight glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G7-8",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-Bia8u"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-7qjJi",
        "type": "genericNode",
        "position": {
          "x": 5880.037853507362,
          "y": 1969.8234951353697
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"Google Generative AI\"\n    description = \"Generate text using Google Generative AI.\"\n    icon = \"GoogleGenerativeAI\"\n    name = \"GoogleGenerativeAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "display_name": "code"
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "learnlm-2.0-flash-experimental",
                  "gemma-3n-e4b-it",
                  "gemma-3-4b-it",
                  "gemma-3-27b-it",
                  "gemma-3-1b-it",
                  "gemma-3-12b-it",
                  "gemini-pro-vision",
                  "gemini-exp-1206",
                  "gemini-2.5-pro-preview-tts",
                  "gemini-2.5-pro-preview-06-05",
                  "gemini-2.5-pro-preview-05-06",
                  "gemini-2.5-pro-preview-03-25",
                  "gemini-2.5-pro-exp-03-25",
                  "gemini-2.5-pro",
                  "gemini-2.5-flash-preview-tts",
                  "gemini-2.5-flash-preview-05-20",
                  "gemini-2.5-flash-preview-04-17-thinking",
                  "gemini-2.5-flash-preview-04-17",
                  "gemini-2.5-flash-lite-preview-06-17",
                  "gemini-2.5-flash",
                  "gemini-2.0-pro-exp-02-05",
                  "gemini-2.0-pro-exp",
                  "gemini-2.0-flash-thinking-exp-1219",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "gemini-2.0-flash-thinking-exp",
                  "gemini-2.0-flash-lite-preview-02-05",
                  "gemini-2.0-flash-lite-preview",
                  "gemini-2.0-flash-lite-001",
                  "gemini-2.0-flash-lite",
                  "gemini-2.0-flash-exp",
                  "gemini-2.0-flash-001",
                  "gemini-2.0-flash",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-pro",
                  "gemini-1.5-flash-latest",
                  "gemini-1.5-flash-8b-latest",
                  "gemini-1.5-flash-8b-001",
                  "gemini-1.5-flash-8b",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-flash",
                  "gemini-1.0-pro-vision-latest"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput"
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Google Generative AI.",
            "icon": "GoogleGenerativeAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LLM2",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "GoogleGenerativeAIModel",
          "id": "GoogleGenerativeAIModel-7qjJi"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 450
        }
      },
      {
        "id": "Prompt-0TzDg",
        "type": "genericNode",
        "position": {
          "x": 3868.043062335021,
          "y": 2512.7487832976276
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-0TzDg",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 9 and 10 as listed in the user’s input (verre_9 and verre_10). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 10 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 8 glasses are present, glasses 9 and 10 must have all values set to [0]; if only 9 glasses are present, glass 10 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 9:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 10:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 9 and 10, based on their order in the document (verre_9 is the 9th glass, verre_10 is the 10th glass).  \n- If the article contains fewer than 10 glasses (e.g., only 8 or 9), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 10 (if present). For glasses 9 and 10 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 9:\" and \"Glass 10:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 9 and Glass 10, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 9 and Glass 10), even if the article has fewer than 9 or 10 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G9-10",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-Ai3sG",
        "type": "genericNode",
        "position": {
          "x": 4258.912445007909,
          "y": 2427.711749390545
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-Ai3sG"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-FrgZ2",
        "type": "genericNode",
        "position": {
          "x": 4654.138445671809,
          "y": 2581.538913173632
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-FrgZ2"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-438mj",
        "type": "genericNode",
        "position": {
          "x": 5086.198990123226,
          "y": 2533.5628161023324
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-438mj"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-KDx27",
        "type": "genericNode",
        "position": {
          "x": 5520.881198190691,
          "y": 2544.9998258379856
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 9 and 10 in the input (9. verre_9 :, 10. verre_10 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n...  \nSearch only for the first ten glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 9 and 10 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n838. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n839. **Nombre de tests** : Number of tests for this glass type\n840. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n841. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n842. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n843. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n844. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n845. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n846. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n847. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n848. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n849. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n850. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n851. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n852. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n853. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n854. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n855. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n856. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n857. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n858. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n859. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n860. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n861. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n862. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n863. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n864. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n865. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n866. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n867. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n868. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n869. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n870. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n871. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n872. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n873. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n874. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n875. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n876. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n877. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n878. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n879. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n880. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n881. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n882. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n883. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n884. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n885. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n886. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n887. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n888. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n889. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n890. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n891. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n892. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n893. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n894. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n895. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n896. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n897. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n898. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n899. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n900. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n901. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n902. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n903. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n904. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n905. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n906. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n907. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n908. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n909. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n910. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n911. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n912. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n913. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n914. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n915. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n916. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n917. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n918. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n919. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n920. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n921. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n922. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n923. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n924. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n925. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n926. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n927. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n928. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]  \n929. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n930. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n931. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n932. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).] \n933. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n934. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n935. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n936. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n937. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n938. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n939. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n940. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n941. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n\n**Expected Format:**\n\nFor each glass type (Verre_type9 to Verre_type10), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n838. Verre_type9 : Type9  \n839. Nombre de tests(Verre_type9) : Number  \n840. Li(Verre_type9) : [value1, value2, ...]  \n841. B(Verre_type9) : [value1, value2, ...]  \n842. O(Verre_type9) : [value1, value2, ...]  \n...  \n941. Congruence(Verre_type9) : [comment1, comment2, ...]  \n\n942. Verre_type10 : Type10  \n943. Nombre de tests(Verre_type10) : Number  \n944. Li(Verre_type10) : [value1, value2, ...]  \n...  \n1045. Congruence(Verre_type10) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 9 and 10 from 838 to 1045, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than ten glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G9-10",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-KDx27"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-Qa1Vw",
        "type": "genericNode",
        "position": {
          "x": 5884.012988740938,
          "y": 2439.7841199148233
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-Qa1Vw"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-QYsZH",
        "type": "genericNode",
        "position": {
          "x": 3851.583302482161,
          "y": 2998.058307591658
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-QYsZH",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 11 and 12 as listed in the user’s input (verre_11 and verre_12). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 12 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 10 glasses are present, glasses 11 and 12 must have all values set to [0]; if only 11 glasses are present, glass 12 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 11:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 12:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 11 and 12, based on their order in the document (verre_11 is the 11th glass, verre_12 is the 12th glass).  \n- If the article contains fewer than 12 glasses (e.g., only 10 or 11), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 12 (if present). For glasses 11 and 12 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 11:\" and \"Glass 12:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 11 and Glass 12, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 11 and Glass 12), even if the article has fewer than 11 or 12 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G11-12",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-vEuJf",
        "type": "genericNode",
        "position": {
          "x": 4250.765475653462,
          "y": 2950.761571843622
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "learnlm-2.0-flash-experimental",
                  "gemma-3n-e4b-it",
                  "gemma-3n-e2b-it",
                  "gemma-3-4b-it",
                  "gemma-3-27b-it",
                  "gemma-3-1b-it",
                  "gemma-3-12b-it",
                  "gemini-pro-vision",
                  "gemini-exp-1206",
                  "gemini-2.5-pro-preview-tts",
                  "gemini-2.5-pro-preview-06-05",
                  "gemini-2.5-pro-preview-05-06",
                  "gemini-2.5-pro-preview-03-25",
                  "gemini-2.5-pro",
                  "gemini-2.5-flash-preview-tts",
                  "gemini-2.5-flash-preview-05-20",
                  "gemini-2.5-flash-lite-preview-06-17",
                  "gemini-2.5-flash-lite",
                  "gemini-2.5-flash",
                  "gemini-2.0-pro-exp-02-05",
                  "gemini-2.0-pro-exp",
                  "gemini-2.0-flash-thinking-exp-1219",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "gemini-2.0-flash-thinking-exp",
                  "gemini-2.0-flash-lite-preview-02-05",
                  "gemini-2.0-flash-lite-preview",
                  "gemini-2.0-flash-lite-001",
                  "gemini-2.0-flash-lite",
                  "gemini-2.0-flash-exp",
                  "gemini-2.0-flash-001",
                  "gemini-2.0-flash",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-pro",
                  "gemini-1.5-flash-latest",
                  "gemini-1.5-flash-8b-latest",
                  "gemini-1.5-flash-8b-001",
                  "gemini-1.5-flash-8b",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-flash",
                  "gemini-1.0-pro-vision-latest"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": false,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": false,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-vEuJf"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 779
        }
      },
      {
        "id": "GlassCompositionConverter-s1NNy",
        "type": "genericNode",
        "position": {
          "x": 4634.995218483277,
          "y": 3074.898839474676
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def get_oxygen_coefficient(self, oxide):\n        \"\"\"Retourne le nombre d'atomes d'oxygène dans la formule oxide.\n        Exemples :\n        - 'SiO2' retourne 2\n        - 'B2O3' retourne 3\n        \"\"\"\n        match = re.search(r'O(\\d+)', oxide)\n        if match:\n            return int(match.group(1))\n        else:\n            # Si la formule ne comporte pas de chiffre après 'O', on suppose 1.\n            return 1\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            # Dictionnaire pour agréger les moles par espèce pour chaque verre.\n            aggregated_moles = {}\n            has_convertible_data = False\n\n            # Parcourir les données pour chaque composant\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                mole_value = None\n\n                # Prioriser mol% oxide\n                if data.get('mol%_oxide', 0) > 0:\n                    mole_value = data['mol%_oxide'] / 100\n                    has_convertible_data = True\n                # Sinon, essayer wt% oxide\n                elif data.get('wt%_oxide', 0) > 0:\n                    if oxide in molar_masses:\n                        mole_value = data['wt%_oxide'] / molar_masses[oxide]\n                        has_convertible_data = True\n                # Sinon, wt% element (attention, cette méthode ne donne pas la part d'oxygène)\n                elif data.get('wt%_element', 0) > 0:\n                    if element in molar_masses:\n                        mole_value = data['wt%_element'] / molar_masses[element]\n                        has_convertible_data = True\n\n                if mole_value is None:\n                    continue\n\n                # Calculer les moles de cation et d’oxygène pour cet oxyde\n                stoich_cation = self.get_stoichiometric_coefficient(oxide, element)\n                stoich_oxygen = self.get_oxygen_coefficient(oxide)\n\n                moles_cation = mole_value * stoich_cation\n                moles_oxygen = mole_value * stoich_oxygen\n\n                # Ajouter aux agrégations\n                aggregated_moles[element] = aggregated_moles.get(element, 0) + moles_cation\n                aggregated_moles['O'] = aggregated_moles.get('O', 0) + moles_oxygen\n\n            # Si on a des données convertibles, calculer la mol%\n            if has_convertible_data:\n                total_moles = sum(aggregated_moles.values())\n                # On ajoute les mol% dans aggregated_moles sous forme de pourcentage\n                for species, moles in aggregated_moles.items():\n                    aggregated_moles[species] = round((moles / total_moles) * 100, 2)\n\n                # Remplacer le contenu de data_by_glass avec les résultats agrégés\n                # Nous créons une liste de dictionnaires pour conserver le même format de sortie\n                data_by_glass[glass] = [{'element': species, 'mol%_element': aggregated_moles[species]} for species in aggregated_moles]\n\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-s1NNy"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-QVHkJ",
        "type": "genericNode",
        "position": {
          "x": 5067.055762934695,
          "y": 3026.9227424033775
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-QVHkJ"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-YXgUi",
        "type": "genericNode",
        "position": {
          "x": 5501.73797100216,
          "y": 3038.3597521390307
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 11 and 12 in the input (11. verre_11 :, 12. verre_12 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n...  \nSearch only for the first twelve glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 11 and 12 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n1046. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n1047. **Nombre de tests** : Number of tests for this glass type\n1048. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n1049. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n1050. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n1051. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n1052. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n1053. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n1054. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n1055. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n1056. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n1057. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n1058. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n1059. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n1060. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1061. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1062. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n1063. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n1064. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n1065. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1066. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n1067. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n1068. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n1069. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n1070. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n1071. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1072. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1073. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n1074. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n1075. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1076. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n1077. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n1078. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n1079. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n1080. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1081. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n1082. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1083. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n1084. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1085. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1086. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n1087. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n1088. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n1089. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n1090. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n1091. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n1092. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n1093. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n1094. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n1095. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n1096. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n1097. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n1098. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n1099. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n1100. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n1101. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1102. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n1103. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n1104. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n1105. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1106. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n1107. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1108. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1109. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n1110. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n1111. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n1112. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1113. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n1114. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n1115. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n1116. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n1117. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n1118. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1119. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n1120. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n1121. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n1122. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n1123. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1124. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n1125. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n1126. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n1127. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n1128. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n1129. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n1130. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n1131. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n1132. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n1133. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n1134. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1135. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n1136. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60] \n1137. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n1138. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1139. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n1140. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]  \n1141. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n1142. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1143. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n1144. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1145. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1146. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1147. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1148. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1149. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type11 to Verre_type12), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n1046. Verre_type11 : Type11  \n1047. Nombre de tests(Verre_type11) : Number  \n1048. Li(Verre_type11) : [value1, value2, ...]  \n1049. B(Verre_type11) : [value1, value2, ...]  \n1050. O(Verre_type11) : [value1, value2, ...]  \n...  \n1149. Congruence(Verre_type11) : [comment1, comment2, ...]  \n\n1150. Verre_type12 : Type12  \n1151. Nombre de tests(Verre_type12) : Number  \n1152. Li(Verre_type12) : [value1, value2, ...]  \n...  \n1253. Congruence(Verre_type12) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 11 and 12 from 1046 to 1253, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twelve glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G11-12",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-YXgUi"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-HTcbf",
        "type": "genericNode",
        "position": {
          "x": 5883.4047596546525,
          "y": 2947.8132379808435
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-HTcbf"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-YSKtu",
        "type": "genericNode",
        "position": {
          "x": 3859.21357800102,
          "y": 3455.522916876403
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-YSKtu",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 13 and 14 as listed in the user’s input (verre_13 and verre_14). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 14 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 12 glasses are present, glasses 13 and 14 must have all values set to [0]; if only 13 glasses are present, glass 14 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 13:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 14:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 13 and 14, based on their order in the document (verre_13 is the 13th glass, verre_14 is the 14th glass).  \n- If the article contains fewer than 14 glasses (e.g., only 12 or 13), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 14 (if present). For glasses 13 and 14 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 13:\" and \"Glass 14:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 13 and Glass 14, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 13 and Glass 14), even if the article has fewer than 13 or 14 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G13-14",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-AdnE6",
        "type": "genericNode",
        "position": {
          "x": 4256.680715374358,
          "y": 3436.463429973836
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-AdnE6"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-TZNTF",
        "type": "genericNode",
        "position": {
          "x": 4649.236249661187,
          "y": 3531.382165734491
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-TZNTF"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-skF0K",
        "type": "genericNode",
        "position": {
          "x": 5077.3695057892255,
          "y": 3476.336949681108
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-skF0K"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-7lvWp",
        "type": "genericNode",
        "position": {
          "x": 5512.05171385669,
          "y": 3487.773959416761
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 13 and 14 in the input (13. verre_13 :, 14. verre_14 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n...  \nSearch only for the first fourteen glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 13 and 14 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n1254. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n1255. **Nombre de tests** : Number of tests for this glass type\n1256. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n1257. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n1258. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n1259. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n1260. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n1261. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n1262. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n1263. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n1264. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n1265. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n1266. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n1267. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n1268. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1269. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1270. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n1271. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n1272. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n1273. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1274. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n1275. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n1276. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n1277. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n1278. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n1279. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1280. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1281. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n1282. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n1283. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1284. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n1285. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n1286. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n1287. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n1288. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1289. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n1290. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1291. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n1292. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1293. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1294. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n1295. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n1296. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n1297. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n1298. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n1299. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n1300. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n1301. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n1302. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n1303. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n1304. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n1305. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n1306. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n1307. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n1308. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n1309. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1310. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n1311. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n1312. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n1313. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1314. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n1315. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1316. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1317. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n1318. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n1319. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n1320. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1321. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n1322. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n1323. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n1324. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n1325. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n1326. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1327. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n1328. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n1329. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n1330. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n1331. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1332. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n1333. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n1334. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n1335. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n1336. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n1337. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n1338. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n1339. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n1340. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n1341. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n1342. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1343. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n1344. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n1345. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n1346. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1347. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n1348. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n1349. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n1350. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1351. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n1352. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1353. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1354. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1355. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1356. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1357. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type13 to Verre_type14), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n1254. Verre_type13 : Type13  \n1255. Nombre de tests(Verre_type13) : Number  \n1256. Li(Verre_type13) : [value1, value2, ...]  \n1257. B(Verre_type13) : [value1, value2, ...]  \n1258. O(Verre_type13) : [value1, value2, ...]  \n...  \n1357. Congruence(Verre_type13) : [comment1, comment2, ...]  \n\n1358. Verre_type14 : Type14  \n1359. Nombre de tests(Verre_type14) : Number  \n1360. Li(Verre_type14) : [value1, value2, ...]  \n...  \n1461. Congruence(Verre_type14) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 13 and 14 from 1254 to 1461, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than fourteen glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G13-14",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-7lvWp"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-X88mX",
        "type": "genericNode",
        "position": {
          "x": 5882.268867986583,
          "y": 3426.407188572827
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-X88mX"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-aYNdd",
        "type": "genericNode",
        "position": {
          "x": 3856.9424711562237,
          "y": 3927.626017073409
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-aYNdd",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 15 and 16 as listed in the user’s input (verre_15 and verre_16). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 16 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 14 glasses are present, glasses 15 and 16 must have all values set to [0]; if only 15 glasses are present, glass 16 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 15:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 16:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 15 and 16, based on their order in the document (verre_15 is the 15th glass, verre_16 is the 16th glass).  \n- If the article contains fewer than 16 glasses (e.g., only 14 or 15), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 16 (if present). For glasses 15 and 16 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 15:\" and \"Glass 16:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 15 and Glass 16, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 15 and Glass 16), even if the article has fewer than 15 or 16 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G15-16",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-tyjTA",
        "type": "genericNode",
        "position": {
          "x": 4256.608860096379,
          "y": 3873.3785051017667
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-tyjTA"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-kzRW8",
        "type": "genericNode",
        "position": {
          "x": 4643.037854493011,
          "y": 3996.4161469494134
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-kzRW8"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-BONhX",
        "type": "genericNode",
        "position": {
          "x": 5075.0983989444285,
          "y": 3948.4400498781138
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-BONhX"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-a7YLm",
        "type": "genericNode",
        "position": {
          "x": 5480.498999631207,
          "y": 3954.2996105888747
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 15 and 16 in the input (15. verre_15 :, 16. verre_16 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n...  \nSearch only for the first sixteen glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 15 and 16 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n1462. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n1463. **Nombre de tests** : Number of tests for this glass type\n1464. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n1465. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n1466. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n1467. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n1468. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n1469. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n1470. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n1471. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n1472. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n1473. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n1474. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n1475. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n1476. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1477. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1478. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n1479. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n1480. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n1481. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1482. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n1483. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n1484. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n1485. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n1486. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n1487. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1488. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1489. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n1490. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n1491. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1492. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n1493. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n1494. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n1495. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n1496. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1497. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n1498. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1499. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n1500. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1501. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1502. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n1503. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n1504. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n1505. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n1506. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n1507. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n1508. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n1509. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n1510. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n1511. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n1512. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n1513. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n1514. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n1515. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n1516. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n1517. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1518. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n1519. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n1520. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n1521. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1522. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n1523. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1524. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1525. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n1526. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n1527. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n1528. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1529. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n1530. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n1531. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n1532. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n1533. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n1534. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1535. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n1536. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n1537. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n1538. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n1539. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1540. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n1541. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n1542. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n1543. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n1544. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n1545. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n1546. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n1547. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n1548. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n1549. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n1550. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1551. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n1552. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]  \n1553. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n1554. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1555. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n1556. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n1557. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n1558. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1559. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n1560. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1561. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1562. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1563. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1564. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1565. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type15 to Verre_type16), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n1462. Verre_type15 : Type15  \n1463. Nombre de tests(Verre_type15) : Number  \n1464. Li(Verre_type15) : [value1, value2, ...]  \n1465. B(Verre_type15) : [value1, value2, ...]  \n1466. O(Verre_type15) : [value1, value2, ...]  \n...  \n1565. Congruence(Verre_type15) : [comment1, comment2, ...]  \n\n1566. Verre_type16 : Type16  \n1567. Nombre de tests(Verre_type16) : Number  \n1568. Li(Verre_type16) : [value1, value2, ...]  \n...  \n1669. Congruence(Verre_type16) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 15 and 16 from 1462 to 1669, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than sixteen glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G15-16",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-a7YLm"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-ry2Rb",
        "type": "genericNode",
        "position": {
          "x": 5881.233483182311,
          "y": 3897.423460386021
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-ry2Rb"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-yPJsv",
        "type": "genericNode",
        "position": {
          "x": 3834.6641101430987,
          "y": 4431.351883148518
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-yPJsv",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 17 and 18 as listed in the user’s input (verre_17 and verre_18). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 18 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 16 glasses are present, glasses 17 and 18 must have all values set to [0]; if only 17 glasses are present, glass 18 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 17:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 18:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 17 and 18, based on their order in the document (verre_17 is the 17th glass, verre_18 is the 18th glass).  \n- If the article contains fewer than 18 glasses (e.g., only 16 or 17), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 18 (if present). For glasses 17 and 18 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 17:\" and \"Glass 18:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 17 and Glass 18, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 17 and Glass 18), even if the article has fewer than 17 or 18 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G17-18",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-dGFo2",
        "type": "genericNode",
        "position": {
          "x": 4252.0354536307905,
          "y": 4359.510358642338
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-dGFo2"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-zZa6D",
        "type": "genericNode",
        "position": {
          "x": 4659.801105964179,
          "y": 4500.1420130245215
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-zZa6D"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-2XThH",
        "type": "genericNode",
        "position": {
          "x": 5052.820037931304,
          "y": 4452.165915953223
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-2XThH"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-Hrgu7",
        "type": "genericNode",
        "position": {
          "x": 5487.502245998769,
          "y": 4463.602925688876
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 17 and 18 in the input (17. verre_17 :, 18. verre_18 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n...  \nSearch only for the first eighteen glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 17 and 18 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n1670. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n1671. **Nombre de tests** : Number of tests for this glass type\n1672. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n1673. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n1674. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n1675. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n1676. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n1677. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n1678. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n1679. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n1680. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n1681. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n1682. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n1683. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n1684. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1685. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1686. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n1687. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n1688. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n1689. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1690. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n1691. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n1692. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n1693. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n1694. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n1695. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1696. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1697. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n1698. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n1699. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1700. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n1701. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n1702. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n1703. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n1704. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1705. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n1706. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1707. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n1708. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1709. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1710. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n1711. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n1712. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n1713. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n1714. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n1715. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n1716. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n1717. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n1718. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n1719. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n1720. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n1721. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n1722. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n1723. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n1724. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n1725. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1726. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n1727. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n1728. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n1729. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1730. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n1731. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1732. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1733. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n1734. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n1735. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n1736. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1737. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n1738. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n1739. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n1740. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n1741. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n1742. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1743. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n1744. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n1745. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n1746. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n1747. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1748. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n1749. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n1750. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n1751. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n1752. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n1753. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n1754. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n1755. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n1756. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n1757. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n1758. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1759. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n1760. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n1761. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n1762. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1763. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n1764. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n1765. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n1766. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1767. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n1768. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1769. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1770. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1771. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1772. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1773. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type17 to Verre_type18), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n1670. Verre_type17 : Type17  \n1671. Nombre de tests(Verre_type17) : Number  \n1672. Li(Verre_type17) : [value1, value2, ...]  \n1673. B(Verre_type17) : [value1, value2, ...]  \n1674. O(Verre_type17) : [value1, value2, ...]  \n...  \n1773. Congruence(Verre_type17) : [comment1, comment2, ...]  \n\n1774. Verre_type18 : Type18  \n1775. Nombre de tests(Verre_type18) : Number  \n1776. Li(Verre_type18) : [value1, value2, ...]  \n...  \n1877. Congruence(Verre_type18) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 17 and 18 from 1670 to 1877, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than eighteen glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G17-18",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-Hrgu7"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-oTIYg",
        "type": "genericNode",
        "position": {
          "x": 5882.364544052165,
          "y": 4365.733087627493
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-oTIYg"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-QOkbs",
        "type": "genericNode",
        "position": {
          "x": 3844.9778529976293,
          "y": 4880.766090426248
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-QOkbs",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 19 and 20 as listed in the user’s input (verre_19 and verre_20). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 20 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 18 glasses are present, glasses 19 and 20 must have all values set to [0]; if only 19 glasses are present, glass 20 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 19:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 20:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 19 and 20, based on their order in the document (verre_19 is the 19th glass, verre_20 is the 20th glass).  \n- If the article contains fewer than 20 glasses (e.g., only 18 or 19), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 20 (if present). For glasses 19 and 20 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 19:\" and \"Glass 20:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 19 and Glass 20, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 19 and Glass 20), even if the article has fewer than 19 or 20 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G19-20",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-EIUXq",
        "type": "genericNode",
        "position": {
          "x": 4251.241996638237,
          "y": 4804.5260627864345
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-EIUXq"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-sufyN",
        "type": "genericNode",
        "position": {
          "x": 4631.073236334417,
          "y": 4949.556220302253
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-sufyN"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-pJSlb",
        "type": "genericNode",
        "position": {
          "x": 5063.1337807858345,
          "y": 4901.580123230953
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-pJSlb"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-uyXpe",
        "type": "genericNode",
        "position": {
          "x": 5497.815988853299,
          "y": 4913.017132966606
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 19 and 20 in the input (19. verre_19 :, 20. verre_20 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n...  \nSearch only for the first twenty glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 19 and 20 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n1878. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n1879. **Nombre de tests** : Number of tests for this glass type\n1880. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n1881. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n1882. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n1883. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n1884. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n1885. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n1886. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n1887. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n1888. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n1889. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n1890. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n1891. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n1892. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1893. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1894. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n1895. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n1896. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n1897. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1898. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n1899. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n1900. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n1901. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n1902. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n1903. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1904. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1905. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n1906. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n1907. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1908. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n1909. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n1910. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n1911. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n1912. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1913. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n1914. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1915. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n1916. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1917. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1918. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n1919. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n1920. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n1921. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n1922. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n1923. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n1924. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n1925. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n1926. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n1927. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n1928. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n1929. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n1930. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n1931. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n1932. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n1933. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n1934. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n1935. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n1936. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n1937. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n1938. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n1939. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n1940. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n1941. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n1942. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n1943. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n1944. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n1945. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n1946. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n1947. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n1948. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n1949. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n1950. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1951. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n1952. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n1953. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n1954. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n1955. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1956. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n1957. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n1958. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n1959. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n1960. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n1961. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n1962. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n1963. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n1964. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n1965. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n1966. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1967. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n1968. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]  \n1969. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n1970. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1971. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n1972. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]\n1973. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n1974. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1975. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n1976. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1977. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1978. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1979. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n1980. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n1981. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type19 to Verre_type20), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n1860. Verre_type19 : Type19  \n1861. Nombre de tests(Verre_type19) : Number  \n1862. Li(Verre_type19) : [value1, value2, ...]  \n1863. B(Verre_type19) : [value1, value2, ...]  \n1864. O(Verre_type19) : [value1, value2, ...]  \n...  \n1932. Densité(Verre_type19) : [value1, value2, ...]  \n1933. Homogénéité(Verre_type19) : [comment1, comment2, ...]  \n...  \n1963. Congruence(Verre_type19) : [comment1, comment2, ...]  \n\n1964. Verre_type20 : Type20  \n1965. Nombre de tests(Verre_type20) : Number  \n1966. Li(Verre_type20) : [value1, value2, ...]  \n...  \n2034. Densité(Verre_type20) : [value1, value2, ...]  \n2035. Homogénéité(Verre_type20) : [comment1, comment2, ...]  \n...  \n2065. Congruence(Verre_type20) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules :**\n\n- Provide all values for the next two glass types (verre_19 and verre_20) from 1860 to 2065, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twenty glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1. seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit may be g.m⁻².d⁻¹ or µm.d⁻¹ (anything commensurable with thickness per unit time; convert if necessary), if not mentioned, calculate it from available data]  \n\n",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G19-20",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-uyXpe"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-itbVX",
        "type": "genericNode",
        "position": {
          "x": 5881.738805221876,
          "y": 4868.5981255623155
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-itbVX"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-pseV4",
        "type": "genericNode",
        "position": {
          "x": 3842.706746152833,
          "y": 5352.869190623254
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-pseV4",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 21 and 22 as listed in the user’s input (verre_21 and verre_22). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 22 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 20 glasses are present, glasses 21 and 22 must have all values set to [0]; if only 21 glasses are present, glass 22 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 21:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 22:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 21 and 22, based on their order in the document (verre_21 is the 21st glass, verre_22 is the 22nd glass).  \n- If the article contains fewer than 22 glasses (e.g., only 20 or 21), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 22 (if present). For glasses 21 and 22 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 21:\" and \"Glass 22:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 21 and Glass 22, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 21 and Glass 22), even if the article has fewer than 21 or 22 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G21-22",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-ZfQia",
        "type": "genericNode",
        "position": {
          "x": 4244.572386659806,
          "y": 5278.828414550258
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-ZfQia"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-9R3Fe",
        "type": "genericNode",
        "position": {
          "x": 4628.802129489621,
          "y": 5421.659320499259
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-9R3Fe"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-p56dl",
        "type": "genericNode",
        "position": {
          "x": 5060.862673941038,
          "y": 5373.683223427959
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-p56dl"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-DpcNf",
        "type": "genericNode",
        "position": {
          "x": 5495.544882008503,
          "y": 5385.120233163612
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 21 and 22 in the input (21. verre_21 :, 22. verre_22 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n21. verre_21 :  \n22. verre_22 :  \n...  \nSearch only for the first twenty-two glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 21 and 22 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n2086. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n2087. **Nombre de tests** : Number of tests for this glass type\n2088. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n2089. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n2090. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n2091. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n2092. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n2093. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n2094. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n2095. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n2096. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n2097. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n2098. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n2099. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n2100. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2101. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2102. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n2103. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n2104. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n2105. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2106. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n2107. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n2108. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n2109. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n2110. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n2111. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2112. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2113. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n2114. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n2115. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2116. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n2117. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n2118. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n2119. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n2120. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2121. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n2122. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2123. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n2124. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2125. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2126. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n2127. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n2128. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n2129. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n2130. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n2131. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n2132. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n2133. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n2134. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n2135. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n2136. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n2137. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n2138. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n2139. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n2140. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n2141. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2142. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n2143. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n2144. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n2145. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2146. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n2147. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2148. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2149. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n2150. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n2151. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n2152. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2153. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n2154. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n2155. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n2156. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n2157. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n2158. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2159. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n2160. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n2161. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n2162. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n2163. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2164. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n2165. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n2166. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n2167. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n2168. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n2169. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n2170. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n2171. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n2172. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n2173. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n2174. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2175. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n2176. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n2177. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n2178. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2179. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n2180. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).] \n2181. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n2182. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2183. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n2184. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2185. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2186. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2187. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2188. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2189. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type1 to Verre_type2), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n2086. Verre_type1 : Type1\n2087. Nombre de tests(Verre_type1) : Number\n2088. Li(Verre_type1) : [value1, value2, ...]  \n2089. B(Verre_type1) : [value1, value2, ...]  \n2090. O(Verre_type1) : [value1, value2, ...]  \n...  \n2189. Congruence(Verre_type1) : [comment1, comment2, ...]  \n\n2190. Verre_type2 : [Type2]  \n2191. Nombre de tests(Verre_type2) : [Number]  \n2192. Li(Verre_type2) : [value1, value2, ...]  \n...  \n2293. Congruence(Verre_type2) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 21 and 22 from 2086 to 2293, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twenty-two glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G21-22",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-DpcNf"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-SL95I",
        "type": "genericNode",
        "position": {
          "x": 5867.79465849907,
          "y": 5365.732114495586
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-SL95I"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-Vuct4",
        "type": "genericNode",
        "position": {
          "x": 3826.246986299973,
          "y": 5838.178714917285
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-Vuct4",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 23 and 24 as listed in the user’s input (verre_23 and verre_24). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 24 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 22 glasses are present, glasses 23 and 24 must have all values set to [0]; if only 23 glasses are present, glass 24 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 23:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 24:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 23 and 24, based on their order in the document (verre_23 is the 23rd glass, verre_24 is the 24th glass).  \n- If the article contains fewer than 24 glasses (e.g., only 22 or 23), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 24 (if present). For glasses 23 and 24 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 23:\" and \"Glass 24:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 23 and Glass 24, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 23 and Glass 24), even if the article has fewer than 23 or 24 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G23-24",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-jtUke",
        "type": "genericNode",
        "position": {
          "x": 4234.2261657385425,
          "y": 5829.36888158855
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-pro-exp-03-25",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-jtUke"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-vOZgf",
        "type": "genericNode",
        "position": {
          "x": 4609.658902301089,
          "y": 5915.019246800302
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-vOZgf"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-HtGPx",
        "type": "genericNode",
        "position": {
          "x": 5041.719446752506,
          "y": 5867.043149729005
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-HtGPx"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-MsQev",
        "type": "genericNode",
        "position": {
          "x": 5476.401654819971,
          "y": 5878.480159464656
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 23 and 24 in the input (23. verre_23 :, 24. verre_24 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n21. verre_21 :  \n22. verre_22 :  \n23. verre_23 :  \n24. verre_24 :  \n...  \nSearch only for the first twenty-four glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 23 and 24 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n2294. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n2295. **Nombre de tests** : Number of tests for this glass type\n2296. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n2297. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n2298. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n2299. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n2300. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n2301. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n2302. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n2303. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n2304. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n2305. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n2306. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n2307. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n2308. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2309. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2310. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n2311. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n2312. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n2313. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2314. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n2315. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n2316. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n2317. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n2318. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n2319. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2320. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2321. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n2322. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n2323. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2324. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n2325. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n2326. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n2327. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n2328. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2329. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n2330. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2331. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n2332. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2333. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2334. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n2335. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n2336. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n2337. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n2338. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n2339. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n2340. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n2341. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n2342. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n2343. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n2344. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n2345. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n2346. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n2347. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n2348. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n2349. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2350. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n2351. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n2352. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n2353. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2354. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n2355. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2356. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2357. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n2358. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n2359. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n2360. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2361. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n2362. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n2363. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n2364. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n2365. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n2366. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2367. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n2368. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n2369. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n2370. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n2371. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2372. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n2373. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n2374. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n2375. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n2376. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n2377. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n2378. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n2379. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n2380. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n2381. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n2382. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2383. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n2384. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n2385. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n2386. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2387. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n2388. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).] \n2389. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n2390. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2391. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n2392. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2393. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2394. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2395. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2396. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2397. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type21 to Verre_type22), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n2294. Verre_type21 : Type21  \n2295. Nombre de tests(Verre_type21) : Number  \n2296. Li(Verre_type21) : [value1, value2, ...]  \n2297. B(Verre_type21) : [value1, value2, ...]  \n2298. O(Verre_type21) : [value1, value2, ...]  \n...  \n2397. Congruence(Verre_type21) : [comment1, comment2, ...]  \n\n2398. Verre_type22 : Type22  \n2399. Nombre de tests(Verre_type22) : Number  \n2400. Li(Verre_type22) : [value1, value2, ...]  \n...  \n2501. Congruence(Verre_type22) : [comment1, comment2, ...]  \n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 23 and 24 from 2294 to 2501, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twenty-four glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G23-24",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-MsQev"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-F9HPH",
        "type": "genericNode",
        "position": {
          "x": 5868.520316278089,
          "y": 5813.406222499151
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-pro-exp-03-25",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-F9HPH"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-zKzbb",
        "type": "genericNode",
        "position": {
          "x": 3833.877261818832,
          "y": 6295.643324202028
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-zKzbb",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 25 and 26 as listed in the user’s input (verre_25 and verre_26). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 26 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 24 glasses are present, glasses 25 and 26 must have all values set to [0]; if only 25 glasses are present, glass 26 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 25:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 26:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 25 and 26, based on their order in the document (verre_25 is the 25th glass, verre_26 is the 26th glass).  \n- If the article contains fewer than 26 glasses (e.g., only 24 or 25), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 26 (if present). For glasses 25 and 26 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 25:\" and \"Glass 26:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 25 and Glass 26, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 25 and Glass 26), even if the article has fewer than 25 or 26 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G25-26",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-1vipg",
        "type": "genericNode",
        "position": {
          "x": 4224.746644491719,
          "y": 6206.207787161312
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-pro-exp-03-25",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-1vipg"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-pCf0i",
        "type": "genericNode",
        "position": {
          "x": 4619.972645155619,
          "y": 6364.433454078033
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-pCf0i"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-dWWyZ",
        "type": "genericNode",
        "position": {
          "x": 5090.865008476517,
          "y": 6368.795895482989
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-dWWyZ"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-IDdkJ",
        "type": "genericNode",
        "position": {
          "x": 5486.715397674501,
          "y": 6327.894366742388
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 25 and 26 in the input (25. verre_25 :, 26. verre_26 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n21. verre_21 :  \n22. verre_22 :  \n23. verre_23 :  \n24. verre_24 :  \n25. verre_25 :  \n26. verre_26 :  \n...  \nSearch only for the first twenty-six glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 25 and 26 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n2502. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n2503. **Nombre de tests** : Number of tests for this glass type\n2504. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n2505. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n2506. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n2507. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n2508. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n2509. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n2510. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n2511. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n2512. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n2513. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n2514. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n2515. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n2516. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2517. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2518. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n2519. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n2520. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n2521. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2522. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n2523. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n2524. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n2525. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n2526. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n2527. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2528. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2529. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n2530. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n2531. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2532. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n2533. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n2534. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n2535. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n2536. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2537. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n2538. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2539. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n2540. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2541. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2542. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n2543. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n2544. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n2545. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n2546. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n2547. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n2548. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n2549. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n2550. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n2551. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n2552. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n2553. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n2554. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n2555. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n2556. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n2557. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2558. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n2559. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n2560. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n2561. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2562. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n2563. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2564. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2565. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n2566. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n2567. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n2568. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2569. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n2570. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n2571. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n2572. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n2573. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n2574. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2575. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n2576. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n2577. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n2578. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n2579. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2580. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n2581. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n2582. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n2583. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n2584. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n2585. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n2586. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n2587. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n2588. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n2589. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n2590. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2591. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n2592. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60] \n2593. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n2594. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2595. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n2596. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).] \n2597. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n2598. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2599. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n2600. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2601. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2602. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2603. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2604. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2605. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n**Expected Format:**\n\nFor each glass type (Verre_type25 to Verre_type26), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n2502. Verre_type25 : Type25\n2503. Nombre de tests(Verre_type25) : Number\n2504. Li(Verre_type25) : [value1, value2, ...]\n2505. B(Verre_type25) : [value1, value2, ...]\n2506. O(Verre_type25) : [value1, value2, ...]\n...\n2605. Congruence(Verre_type25) : [comment1, comment2, ...]\n\n2606. Verre_type26 : Type26\n2607. Nombre de tests(Verre_type26) : Number\n2608. Li(Verre_type26) : [value1, value2, ...]\n...\n2709. Congruence(Verre_type26) : [comment1, comment2, ...]\n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 25 and 26 from 2502 to 2709, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twenty-six glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G25-26",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-IDdkJ"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-JwW1V",
        "type": "genericNode",
        "position": {
          "x": 5892.460401263453,
          "y": 6337.787855455048
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-JwW1V"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-R5RgD",
        "type": "genericNode",
        "position": {
          "x": 3831.6061549740357,
          "y": 6767.746424399036
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-R5RgD",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 27 and 28 as listed in the user’s input (verre_27 and verre_28). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 28 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 26 glasses are present, glasses 27 and 28 must have all values set to [0]; if only 27 glasses are present, glass 28 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 27:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 28:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 27 and 28, based on their order in the document (verre_27 is the 27th glass, verre_28 is the 28th glass).  \n- If the article contains fewer than 28 glasses (e.g., only 26 or 27), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 28 (if present). For glasses 27 and 28 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 27:\" and \"Glass 28:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 27 and Glass 28, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 27 and Glass 28), even if the article has fewer than 27 or 28 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G27-28",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-Tkx8P",
        "type": "genericNode",
        "position": {
          "x": 4226.874040780557,
          "y": 6672.812758441276
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-pro-exp-03-25",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-Tkx8P"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GlassCompositionConverter-7yzWp",
        "type": "genericNode",
        "position": {
          "x": 4617.701538310823,
          "y": 6836.5365542750405
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-7yzWp"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-FU0VT",
        "type": "genericNode",
        "position": {
          "x": 5049.762082762241,
          "y": 6788.560457203741
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-FU0VT"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-mzw6H",
        "type": "genericNode",
        "position": {
          "x": 5484.444290829706,
          "y": 6799.997466939392
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 27 and 28 in the input (27. verre_27 :, 28. verre_28 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n21. verre_21 :  \n22. verre_22 :  \n23. verre_23 :  \n24. verre_24 :  \n25. verre_25 :  \n26. verre_26 :  \n27. verre_27 :  \n28. verre_28 :  \n...  \nSearch only for the first twenty-eight glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 27 and 28 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n2710. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n2711. **Nombre de tests** : Number of tests for this glass type\n2712. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n2713. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n2714. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n2715. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n2716. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n2717. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n2718. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n2719. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n2720. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n2721. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n2722. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n2723. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n2724. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2725. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2726. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n2727. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n2728. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n2729. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2730. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n2731. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n2732. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n2733. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n2734. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n2735. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2736. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2737. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n2738. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n2739. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2740. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n2741. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n2742. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n2743. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n2744. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2745. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n2746. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2747. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n2748. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2749. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2750. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n2751. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n2752. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n2753. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n2754. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n2755. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n2756. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n2757. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n2758. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n2759. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n2760. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n2761. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n2762. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n2763. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n2764. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n2765. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2766. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n2767. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n2768. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n2769. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2770. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n2771. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2772. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2773. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n2774. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n2775. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n2776. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2777. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n2778. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n2779. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n2780. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n2781. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n2782. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2783. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n2784. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n2785. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n2786. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n2787. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2788. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n2789. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n2790. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n2791. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n2792. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n2793. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n2794. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n2795. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n2796. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n2797. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n2798. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2799. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n2800. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n2801. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n2802. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2803. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n2804. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).]  \n2805. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n2806. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2807. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n2808. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2809. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2810. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2811. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n2812. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2813. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type27 to Verre_type28), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n2710. Verre_type27 : Type27\n2711. Nombre de tests(Verre_type27) : Number\n2712. Li(Verre_type27) : [value1, value2, ...]\n2713. B(Verre_type27) : [value1, value2, ...]\n2714. O(Verre_type27) : [value1, value2, ...]\n...\n2813. Congruence(Verre_type27) : [comment1, comment2, ...]\n\n2814. Verre_type28 : Type28\n2815. Nombre de tests(Verre_type28) : Number\n2816. Li(Verre_type28) : [value1, value2, ...]\n...\n2917. Congruence(Verre_type28) : [comment1, comment2, ...]\n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 27 and 28 from 2710 to 2917, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than twenty-eight glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G27-28",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-mzw6H"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-Uhy9L",
        "type": "genericNode",
        "position": {
          "x": 5894.156964822362,
          "y": 6780.008187093639
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-Uhy9L"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "Prompt-OwAmP",
        "type": "genericNode",
        "position": {
          "x": 3821.2692911299027,
          "y": 7190.435270759675
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-OwAmP",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions :**\n\nRead the provided article and extract the composition of glasses 29 and 30 as listed in the user’s input (verre_29 and verre_30). The composition may be expressed in oxides or elements, with units such as wt% oxide, wt% element, wt% cation, mol% oxide, mol% element, mol% cation, or atomic%. Present the composition strictly in the format below, adhering to the rules without deviation. If the article contains fewer than 30 glasses, assign [0] to all composition values for the missing glasses (e.g., if only 28 glasses are present, glasses 29 and 30 must have all values set to [0]; if only 29 glasses are present, glass 30 must have all values set to [0]).\n\n**Document :**  \n{Document}\n\n---\n\n**Expected Format :**\n\nGlass 29:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\nGlass 30:  \n1. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n2. [oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]  \n...  \n\n---\n\n**Strict Rules :**\n\n- Extract only the composition values explicitly provided in the article for glasses 29 and 30, based on their order in the document (verre_29 is the 29th glass, verre_30 is the 30th glass).  \n- If the article contains fewer than 30 glasses (e.g., only 28 or 29), use [0] for all six values ([wt%][oxide], [wt%][element], [wt%][cation], [mol%][oxide], [mol%][element], [mol%][cation]) for each oxide or element listed for the present glass(es). If no glasses are present, list at least one default entry (e.g., [SiO2][Si]) with all six values set to [0].  \n- Include all oxides and elements explicitly mentioned in the article for glasses 1 through 30 (if present). For glasses 29 and 30 not present, replicate the same list of oxides/elements from the earlier glass(es) (if any) and set all six values to [0].  \n- Do not include oxygen ([O][O]) unless it is explicitly listed as a component in the article (e.g., with mol% element or wt% element).  \n- For each oxide mentioned, pair it with its corresponding element (e.g., [SiO2][Si], [Na2O][Na]). For an element mentioned without an oxide, use [element][element] (e.g., [Na][Na]). For oxygen as a component, use [O][O].  \n- Each line must contain exactly six values in this order: [value][wt%][oxide], [value][wt%][element], [value][wt%][cation], [value][mol%][oxide], [value][mol%][element], [value][mol%][cation].  \n- Values should be numbers without any additional symbols. For example, use 0.1 instead of < 0.1.\n- If a value is not provided for a specific unit in the article, use [0] for that value and its unit (e.g., [0][wt%][oxide]).  \n- Every value must be followed by its unit in brackets, and every unit must be followed by its type in brackets (e.g., [55.3][wt%][oxide]).  \n- Do not duplicate, convert, or infer values across units. Use only the exact values provided in the article.  \n- If a value is provided in atomic% for an oxide or element, place it in the [value][mol%][element] position, using the [mol%][element] label. Set all other five positions to [0] unless explicitly provided otherwise.  \n- Each line must follow the exact syntax: `[oxide][element] : [value][wt%][oxide]; [value][wt%][element]; [value][wt%][cation]; [value][mol%][oxide]; [value][mol%][element]; [value][mol%][cation]` with semicolons separating each value-unit pair and a space before and after each semicolon.  \n- Headers must be exactly \"Glass 29:\" and \"Glass 30:\" followed by a newline.  \n- Number each line starting with 1. for each glass, followed by a space, then the composition data.  \n- Do not include comments, explanations, or any text outside the expected format.  \n- Output only the composition data for Glass 29 and Glass 30, nothing else.  \n- If the article provides no composition data for a glass, list at least one default entry (e.g., [SiO2][Si]) with all six values as [0].  \n- Ensure the output always includes exactly two glasses (Glass 29 and Glass 30), even if the article has fewer than 29 or 30 glasses.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G29-30",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "Prompt"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        }
      },
      {
        "id": "GoogleGenerativeAIModel-41cGM",
        "type": "genericNode",
        "position": {
          "x": 4223.134931636876,
          "y": 7202.165305792549
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-pro-exp-03-25",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-41cGM"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        },
        "dragging": false
      },
      {
        "id": "GlassCompositionConverter-2zfLq",
        "type": "genericNode",
        "position": {
          "x": 4607.364674466689,
          "y": 7259.22540063568
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport re\n\nclass GlassCompositionConverter(Component):\n    display_name = \"Glass Composition Converter\"\n    description = \"Converts glass composition from any unit to mol% element, preserving existing mol% element values.\"\n    documentation = \"https://docs.langflow.org/components-custom-components\"\n    icon = \"code\"\n    name = \"GlassCompositionConverter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_text\",\n            display_name=\"Input Text\",\n            info=\"Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Converted Data\", name=\"converted_data\", method=\"process_input\"),\n    ]\n\n    def process_input(self) -> Message:\n        \"\"\"Process the input text, extract data, and convert all units to mol% element.\"\"\"\n        text = self.input_text\n        lines = text.split('\\n')\n        current_glass = None\n        data_by_glass = {}\n\n        # Parse each line\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n\n            # Detect the start of a new glass (English header: \"Glass X:\")\n            if line.startswith(\"Glass\"):\n                current_glass = line.split(\":\")[0].strip()\n                data_by_glass[current_glass] = []\n                continue\n\n            # Parse composition data (lines starting with a number followed by \".\")\n            if current_glass and re.match(r'\\d+\\.', line):\n                data = self.parse_line(line)\n                if data:\n                    data_by_glass[current_glass].append(data)\n\n        # Perform conversions\n        self.convert_compositions(data_by_glass)\n\n        # Generate output text and convert to Message\n        output_text = self.format_output(data_by_glass)\n        message = Message(text=output_text)\n        self.status = message.text  # Update status with the message text\n        return message\n\n    def parse_line(self, line):\n        \"\"\"Parse a line to extract oxide, element, and values.\"\"\"\n        match = re.match(r'\\d+\\. \\[(\\w+)\\]\\[(\\w+)\\] :', line)\n        if not match:\n            return None\n        oxide, element = match.groups()\n\n        data = {'oxide': oxide, 'element': element}\n        parts = line.split(':')[1].strip().split(';')\n        for part in parts:\n            part = part.strip()\n            if part:\n                match = re.search(r'\\[([\\d.]+)\\]\\[(\\w+%)\\]\\[(\\w+)\\]', part)\n                if match:\n                    value, unit, type_ = match.groups()\n                    key = f\"{unit}_{type_}\"\n                    data[key] = float(value) if value else 0\n                else:\n                    raise ValueError(f\"Invalid part format: {part}\")\n        return data\n\n    def get_stoichiometric_coefficient(self, oxide, element):\n        \"\"\"Return the stoichiometric coefficient of the element in the oxide.\"\"\"\n        match = re.match(rf'^{element}(\\d*)', oxide)\n        if match:\n            coeff_str = match.group(1)\n            return int(coeff_str) if coeff_str else 1\n        return 1  # Default to 1 if not found\n\n    def convert_compositions(self, data_by_glass):\n        \"\"\"Convert all units (wt% oxide, wt% element, mol% oxide) to mol% element, preserving existing mol% element.\"\"\"\n        molar_masses = {\n            'SiO2': 60.08, 'Si': 28.09,\n            'B2O3': 69.62, 'B': 10.81,\n            'P2O5': 141.94, 'P': 30.97,\n            'TeO2': 159.60, 'Te': 127.60,\n            'GeO2': 104.64, 'Ge': 72.64,\n            'As2O3': 197.84, 'As': 74.92,\n            'Sb2O3': 291.52, 'Sb': 121.76,\n            'Bi2O3': 465.96, 'Bi': 208.98,\n            'V2O5': 181.88, 'V': 50.94,\n            'WO3': 231.84, 'W': 183.84,\n            'Na2O': 61.98, 'Na': 22.99,\n            'K2O': 94.20, 'K': 39.10,\n            'Li2O': 29.88, 'Li': 6.94,\n            'CaO': 56.08, 'Ca': 40.08,\n            'MgO': 40.31, 'Mg': 24.31,\n            'BaO': 153.33, 'Ba': 137.33,\n            'SrO': 103.62, 'Sr': 87.62,\n            'Cs2O': 281.81, 'Cs': 132.91,\n            'Rb2O': 186.94, 'Rb': 85.47,\n            'PbO': 223.20, 'Pb': 207.20,\n            'ZnO': 81.38, 'Zn': 65.38,\n            'CdO': 128.41, 'Cd': 112.41,\n            'Ag2O': 231.74, 'Ag': 107.87,\n            'Tl2O': 424.76, 'Tl': 204.38,\n            'Al2O3': 101.96, 'Al': 26.98,\n            'Fe2O3': 159.69, 'Fe': 55.85,\n            'TiO2': 79.87, 'Ti': 47.87,\n            'ZrO2': 123.22, 'Zr': 91.22,\n            'CeO2': 172.12, 'Ce': 140.12,\n            'La2O3': 325.81, 'La': 138.91,\n            'Nd2O3': 336.48, 'Nd': 144.24,\n            'HfO2': 210.49, 'Hf': 178.49,\n            'SnO2': 150.71, 'Sn': 118.71,\n            'NiO': 74.69, 'Ni': 58.69,\n            'Cr2O3': 152.00, 'Cr': 52.00,\n            'MnO2': 86.94, 'Mn': 54.94,\n            'Y2O3': 225.81, 'Y': 88.91,\n            'Pr2O3': 329.81, 'Pr': 140.91,\n            'Sm2O3': 348.72, 'Sm': 150.36,\n            'Eu2O3': 351.92, 'Eu': 151.96,\n            'Gd2O3': 362.50, 'Gd': 157.25,\n            'Nb2O5': 265.81, 'Nb': 92.91,\n            'Ta2O5': 441.89, 'Ta': 180.95,\n            'ThO2': 264.04, 'Th': 232.04,\n            'Ga2O3': 187.44, 'Ga': 69.72,\n            'In2O3': 277.64, 'In': 114.82,\n            'Tb2O3': 365.85, 'Tb': 158.93,\n            'Dy2O3': 373.00, 'Dy': 162.50,\n            'Er2O3': 382.52, 'Er': 167.26,\n            'Yb2O3': 394.10, 'Yb': 173.05,\n            'Sc2O3': 137.91, 'Sc': 44.96,\n            'Ho2O3': 377.86, 'Ho': 164.93,\n            'Tm2O3': 385.86, 'Tm': 168.93,\n            'Lu2O3': 397.94, 'Lu': 174.97,\n            'UO2': 270.03, 'U': 238.03,\n            'UO3': 286.03, 'U': 238.03,\n            'PuO2': 276.06, 'Pu': 244.06,\n            'NpO2': 269.05, 'Np': 237.05,\n            'Am2O3': 534.12, 'Am': 243.06,\n            'Cm2O3': 542.14, 'Cm': 247.07,\n            'MoO3': 143.95, 'Mo': 95.95,\n            'RuO2': 133.07, 'Ru': 101.07,\n            'SeO2': 110.97, 'Se': 78.97,\n            'CoO': 74.93, 'Co': 58.93,\n            'CuO': 79.55, 'Cu': 63.55,\n            'Au2O3': 441.94, 'Au': 196.97,\n        }\n\n        for glass, compositions in data_by_glass.items():\n            if not compositions:\n                continue\n\n            total_moles_element = 0\n            has_convertible_data = False\n\n            # Calculate total moles of elements from all convertible units\n            for data in compositions:\n                oxide = data['oxide']\n                element = data['element']\n\n                # Prioritize mol% oxide if available\n                mol_oxide = data.get('mol%_oxide', 0)\n                if mol_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% oxide\n                wt_oxide = data.get('wt%_oxide', 0)\n                if wt_oxide > 0:\n                    has_convertible_data = True\n                    if oxide in molar_masses and element in molar_masses:\n                        stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                        moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                        total_moles_element += moles_element\n                    continue\n\n                # Otherwise, check wt% element\n                wt_element = data.get('wt%_element', 0)\n                if wt_element > 0:\n                    has_convertible_data = True\n                    if element in molar_masses:\n                        moles_element = wt_element / molar_masses[element]\n                        total_moles_element += moles_element\n\n            # Convert all units to mol% element if convertible data exists\n            if has_convertible_data and total_moles_element > 0:\n                for data in compositions:\n                    element = data['element']\n                    if element in molar_masses:\n                        # Preserve existing mol% element if present\n                        if 'mol%_element' in data and data['mol%_element'] > 0:\n                            continue\n\n                        # Conversion from mol% oxide\n                        mol_oxide = data.get('mol%_oxide', 0)\n                        if mol_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (mol_oxide / 100) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% oxide\n                        wt_oxide = data.get('wt%_oxide', 0)\n                        if wt_oxide > 0:\n                            oxide = data['oxide']\n                            if oxide in molar_masses:\n                                stoichiometric_coeff = self.get_stoichiometric_coefficient(oxide, element)\n                                moles_element = (wt_oxide / molar_masses[oxide]) * stoichiometric_coeff\n                                mol_percent_element = (moles_element / total_moles_element) * 100\n                                data['mol%_element'] = round(mol_percent_element, 2)\n                            continue\n\n                        # Conversion from wt% element\n                        wt_element = data.get('wt%_element', 0)\n                        if wt_element > 0:\n                            moles_element = wt_element / molar_masses[element]\n                            mol_percent_element = (moles_element / total_moles_element) * 100\n                            data['mol%_element'] = round(mol_percent_element, 2)\n\n    def format_output(self, data_by_glass):\n        \"\"\"Format the converted data into readable text.\"\"\"\n        output = []\n        for glass, compositions in data_by_glass.items():\n            if compositions:  # Only if data exists\n                output.append(f\"{glass.lower()}:\")\n                for data in compositions:\n                    element = data['element']\n                    mol_percent = data.get('mol%_element', 0)\n                    if mol_percent > 0:\n                        output.append(f\"mol% {element} = {mol_percent}\")\n        return \"\\n\".join(output)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_text": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_text",
                "value": "",
                "display_name": "Input Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text containing glass composition data from the LLM (e.g., wt% oxide, wt% element, mol% oxide, or mol% element).",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Converts glass composition from any unit to mol% element, preserving existing mol% element values.",
            "icon": "code",
            "base_classes": [
              "Message"
            ],
            "display_name": "Glass Composition Converter",
            "documentation": "https://docs.langflow.org/components-custom-components",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "converted_data",
                "hidden": null,
                "display_name": "Converted Data",
                "method": "process_input",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_text"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "GlassCompositionConverter",
          "id": "GlassCompositionConverter-2zfLq"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        }
      },
      {
        "id": "CombineText-w9zLG",
        "type": "genericNode",
        "position": {
          "x": 5039.425218918108,
          "y": 7211.24930356438
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate two text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the two text inputs. Defaults to a whitespace.\",\n            value=\" \",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        combined = self.delimiter.join([self.text1, self.text2])\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the two text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate two text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-w9zLG"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 334
        }
      },
      {
        "id": "Prompt-7xuDl",
        "type": "genericNode",
        "position": {
          "x": 5474.107426985573,
          "y": 7222.6863133000315
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "**Instructions:**\n\nRead the document below and the input provided by the user. Extract the following information for the glass types listed as 29 and 30 in the input (29. verre_29 :, 30. verre_30 :). Use the mol% element compositions provided in the input for each glass, and replace any unmentioned composition with 0. For all other parameters, search for the information in the document and indicate \"Not available\" if not found. Format the responses clearly according to the requested format, without adding comments or units.\n\nFrom the input:\n1. verre_1 :  \n2. verre_2 :  \n3. verre_3 :  \n4. verre_4 :  \n5. verre_5 :  \n6. verre_6 :  \n7. verre_7 :  \n8. verre_8 :  \n9. verre_9 :  \n10. verre_10 :  \n11. verre_11 :  \n12. verre_12 :  \n13. verre_13 :  \n14. verre_14 :  \n15. verre_15 :  \n16. verre_16 :  \n17. verre_17 :  \n18. verre_18 :  \n19. verre_19 :  \n20. verre_20 :  \n21. verre_21 :  \n22. verre_22 :  \n23. verre_23 :  \n24. verre_24 :  \n25. verre_25 :  \n26. verre_26 :  \n27. verre_27 :  \n28. verre_28 :  \n29. verre_29 :  \n30. verre_30 :  \n...  \nSearch only for the first thirty glass types in the document.\n\n**Document:**  \n{Document}\n\n---\n\n**Requested Information:**\n\nFor the glass types 29 and 30 in the list of names provided by the user, extract the following information. For each parameter, provide a list of values where the length of the list matches the number of tests specified for that glass type in the document. Each value in the list corresponds to a specific test (e.g., for 2 tests, [value1, value2] where value1 is for test1 and value2 is for test2).\n\n2918. **Verre_type** : Description or identifier of the glass type; Description or identifier of the glass type\n2919. **Nombre de tests** : Number of tests for this glass type\n2920. **Li** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Li₂O)]  \n2921. **B** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to B₂O₃)]  \n2922. **O** : [List of numeric values, one per test; use 0 if not found in the input]  \n2923. **Na** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Na₂O)]  \n2924. **Mg** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MgO)]  \n2925. **Al** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Al₂O₃)]  \n2926. **Si** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to SiO₂)]  \n2927. **P** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to P₂O₅)]  \n2928. **K** : [List of numeric values, one per test; use 0 if not found in the input]  \n2929. **Ca** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to CaO)]  \n2930. **Ti** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to TiO₂)]  \n2931. **V** : [List of numeric values, one per test; use 0 if not found in the input]  \n2932. **Cr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2933. **Mn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2934. **Fe** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Fe₂O₃)]  \n2935. **Co** : [List of numeric values, one per test; use 0 if not found in the input]  \n2936. **Ni** : [List of numeric values, one per test; use 0 if not found in the input]  \n2937. **Cu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2938. **Zn** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZnO)]  \n2939. **Ga** : [List of numeric values, one per test; use 0 if not found in the input]  \n2940. **Ge** : [List of numeric values, one per test; use 0 if not found in the input]  \n2941. **As** : [List of numeric values, one per test; use 0 if not found in the input]  \n2942. **Se** : [List of numeric values, one per test; use 0 if not found in the input]  \n2943. **Rb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2944. **Sr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2945. **Y** : [List of numeric values, one per test; use 0 if not found in the input]  \n2946. **Zr** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to ZrO₂)]  \n2947. **Nb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2948. **Mo** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to MoO₃)]  \n2949. **Tc** : [List of numeric values, one per test; use 0 if not found in the input]  \n2950. **Ru** : [List of numeric values, one per test; use 0 if not found in the input]  \n2951. **Rh** : [List of numeric values, one per test; use 0 if not found in the input]  \n2952. **Pd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2953. **Ag** : [List of numeric values, one per test; use 0 if not found in the input]  \n2954. **Cd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2955. **In** : [List of numeric values, one per test; use 0 if not found in the input]  \n2956. **Sn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2957. **Sb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2958. **Te** : [List of numeric values, one per test; use 0 if not found in the input]  \n2959. **I** : [List of numeric values, one per test; use 0 if not found in the input]  \n2960. **Cs** : [List of numeric values, one per test; use 0 if not found in the input]  \n2961. **Ba** : [List of numeric values, one per test; use 0 if not found in the input]  \n2962. **La** : [List of numeric values, one per test; use 0 if not found in the input]  \n2963. **Hf** : [List of numeric values, one per test; use 0 if not found in the input]  \n2964. **Ta** : [List of numeric values, one per test; use 0 if not found in the input]  \n2965. **W** : [List of numeric values, one per test; use 0 if not found in the input]  \n2966. **Re** : [List of numeric values, one per test; use 0 if not found in the input]  \n2967. **Os** : [List of numeric values, one per test; use 0 if not found in the input]  \n2968. **Ir** : [List of numeric values, one per test; use 0 if not found in the input]  \n2969. **Pt** : [List of numeric values, one per test; use 0 if not found in the input]  \n2970. **Au** : [List of numeric values, one per test; use 0 if not found in the input]  \n2971. **Hg** : [List of numeric values, one per test; use 0 if not found in the input]  \n2972. **Tl** : [List of numeric values, one per test; use 0 if not found in the input]  \n2973. **Pb** : [List of numeric values, one per test; use 0 if not found in the input]  \n2974. **Bi** : [List of numeric values, one per test; use 0 if not found in the input]  \n2975. **Po** : [List of numeric values, one per test; use 0 if not found in the input]  \n2976. **At** : [List of numeric values, one per test; use 0 if not found in the input]  \n2977. **Rn** : [List of numeric values, one per test; use 0 if not found in the input]  \n2978. **Ce** : [List of numeric values, one per test; use 0 if not found in the input; Numeric value, if not found set to zero (related to Ce₂O₃)]  \n2979. **Pr** : [List of numeric values, one per test; use 0 if not found in the input]  \n2980. **Nd** : [List of numeric values, one per test; use 0 if not found in the input]  \n2981. **S(autres TR)** : [List of numeric values, one per test; sum of other rare earth elements not listed above, use 0 if not found]  \n2982. **Th** : [List of numeric values, one per test; use 0 if not found in the input]  \n2983. **U** : [List of numeric values, one per test; use 0 if not found in the input]  \n2984. **Pu** : [List of numeric values, one per test; use 0 if not found in the input]  \n2985. **Np** : [List of numeric values, one per test; use 0 if not found in the input]  \n2986. **Am** : [List of numeric values, one per test; use 0 if not found in the input]  \n2987. **Cm** : [List of numeric values, one per test; use 0 if not found in the input]  \n2988. **S(autres An)** : [List of numeric values, one per test; sum of other actinides not listed above, use 0 if not found]  \n2989. **Somme** : [List of numeric values, one per test; sum of the composition values above for each test; Numeric value, calculate the sum of the composition values above]  \n2990. **Densité** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2991. **Homogénéité** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment]  \n2992. **% B(IV)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, fraction or percentage of tetracoordinated boron in the glass]  \n2993. **Irradié** : [List of \"O/N\" values, one per test; if available, otherwise \"Not available\"; Y/N, Was the glass studied for initial rate pre-irradiated?]  \n2994. **Caractéristiques si irradié** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, If the answer to the left column is yes, what are the irradiation characteristics: electrons, which heavy ions (heavy ions or swift ions), what energy, what fluence?]  \n2995. **Température** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n2996. **Statique/dynamique** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, Static = the alteration solution is not renewed. Dynamic = the alteration solution is renewed.]  \n2997. **Plage granulométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: mesh, mesh size, particle size fraction...]  \n2998. **Surface spécifique géométrique (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: geometric surface area]  \n2999. **Surface spécifique BET (si poudre)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: BET must be specified]  \n3000. **Qualité de polissage (si monolithe)** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, terms that may be used: polishing grade]  \n3001. **Masse du verre** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the mass of glass placed in the reactor]  \n3002. **Surface du verre (S)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the surface area of the glass placed in the reactor]  \n3003. **Volume de la solution (V)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the volume of solution in the reactor, do not confuse this surface with the geometric specific surface]  \n3004. **Débit de la solution** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, if the test is performed dynamically, we seek the solution flow rate here. Possible terms are renewal rate, flow rate]  \n3005. **pH initial (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: onset pH, starting pH, pH of the leachate, pH of the solution… We also seek to know if the given value is measured at ambient temperature (this field) or at the test temperature (next field)]  \n3006. **pH initial (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n3007. **Composition de la solution** : [List of comments, one per test; if available, otherwise \"Not available\"; Comment, here we seek to know the chemical composition of the alteration solution]  \n3008. **Durée de l'expérience** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, convert to hours if the unit is not already in hours. For example, if the unit is in days, multiply by 24; if in minutes, divide by 60]\n3009. **pH final (T amb)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for initial pH]  \n3010. **pH final (T essai)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n3011. **Normalisation de la vitesse (Sgeo ou SBET)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek to know if the initial rate is calculated based on geometric surface or BET surface]  \n3012. **V₀(Si) ou r₀(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, here we seek the initial rate (initial dissolution rate, forward rate) determined from Si measured in solution. The unit should be in g.m⁻².d⁻¹ or equivalent (e.g., µm.d⁻¹). If the rate is provided in a different unit or in a transformed form (such as log(rf)), convert it to the specified units. For logarithmic values, assume log is base 10 unless specified otherwise, and compute the rate as 10^log(rf).] \n3013. **r²(Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, possible terms: linear regression coefficient, regression coefficient]  \n3014. **Ordonnée à l'origine (Si)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n3015. **V₀(B) ou r₀(B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, same as for Si]  \n3016. **Ordonnée à l'origine (B)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n3017. **V₀(Na) ou r₀(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n3018. **r²(Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n3019. **Ordonnée à l'origine (Na)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value, terms that may be used: y-intercept]  \n3020. **V₀(ΔM) ou r₀(ΔM)** : [List of numeric values, one per test; if available, otherwise \"Not available\"; Numeric value]  \n3021. **Congruence** : [List of comments or numeric values, one per test; if available, otherwise \"Not available\"; Comment, sometimes congruence may be expressed as a numeric value and may also be in English as \"Congruency (Na/Si)\", so cite this value, e.g., 1.1±0.1, and report the value in the table as it is]  \n\n---\n\n**Expected Format:**\n\nFor each glass type (Verre_type29 to Verre_type30), repeat the information above with incrementing numbers. The number of tests determines the length of the list for each parameter. Each value in the list corresponds to a specific test (e.g., for 2 tests: [value1, value2]). Never associate a single value with multiple tests (e.g., do not use \"value (test1 and test2)\" or \"value (test1 et 2)\"; always use separate values like [value1, value2] even if value1 equals value2).\n\n2918. Verre_type29 : Type29\n2919. Nombre de tests(Verre_type29) : Number\n2920. Li(Verre_type29) : [value1, value2, ...]\n2921. B(Verre_type29) : [value1, value2, ...]\n2922. O(Verre_type29) : [value1, value2, ...]\n...\n3021. Congruence(Verre_type29) : [comment1, comment2, ...]\n\n3022. Verre_type30 : Type30\n3023. Nombre de tests(Verre_type30) : Number\n3024. Li(Verre_type30) : [value1, value2, ...]\n...\n3125. Congruence(Verre_type30) : [comment1, comment2, ...]\n\n---\n\n**Strict Rules:**\n\n- Provide all values for the glass types 29 and 30 from 2918 to 3125, without skipping any parameter.\n- Determine the number of tests for each glass type from the document and ensure every parameter has a list with that exact number of values.\n- For mol% element compositions (Li to S(autres An)), use the values from the input; if not mentioned, use [0, 0, ...] with the length matching the number of tests.\n- For all other parameters, extract values from the document; if not found, use [\"Not available\", \"Not available\", ...] with the length matching the number of tests.\n- Do not add comments, explanations, or units to any value.\n- Never associate a single value with multiple tests; always provide a separate value for each test in the list (e.g., [value1, value2] instead of [value (test1 and test2)]).\n- If the document has fewer than thirty glass types, use \"Not available\" for the missing glass type’s Verre_type and set Nombre de tests to 0, with all subsequent lists as [0] or [\"Not available\"] of length 1.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "tool_placeholder": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_placeholder",
                "value": "",
                "display_name": "Tool Placeholder",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "Document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "Document",
                "display_name": "Document",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt_G29-30",
            "documentation": "",
            "minimized": false,
            "custom_fields": {
              "template": [
                "Document"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "Prompt",
          "id": "Prompt-7xuDl"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 340
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-evcyx",
        "type": "genericNode",
        "position": {
          "x": 5888.218604111863,
          "y": 7257.678322624708
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-evcyx"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GoogleGenerativeAIModel-NTWiN",
        "type": "genericNode",
        "position": {
          "x": 4243.805336953997,
          "y": 1563.48712073885
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.23,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-NTWiN"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "GoogleGenerativeAIModel-Y5uD2",
        "type": "genericNode",
        "position": {
          "x": 4224.380021913577,
          "y": 611.253868584012
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 100000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "learnlm-2.0-flash-experimental",
                  "gemma-3n-e4b-it",
                  "gemma-3n-e2b-it",
                  "gemma-3-4b-it",
                  "gemma-3-27b-it",
                  "gemma-3-1b-it",
                  "gemma-3-12b-it",
                  "gemini-pro-vision",
                  "gemini-exp-1206",
                  "gemini-2.5-pro-preview-tts",
                  "gemini-2.5-pro-preview-06-05",
                  "gemini-2.5-pro-preview-05-06",
                  "gemini-2.5-pro-preview-03-25",
                  "gemini-2.5-pro",
                  "gemini-2.5-flash-preview-tts",
                  "gemini-2.5-flash-preview-05-20",
                  "gemini-2.5-flash-preview-04-17-thinking",
                  "gemini-2.5-flash-preview-04-17",
                  "gemini-2.5-flash-lite-preview-06-17",
                  "gemini-2.5-flash",
                  "gemini-2.0-pro-exp-02-05",
                  "gemini-2.0-pro-exp",
                  "gemini-2.0-flash-thinking-exp-1219",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "gemini-2.0-flash-thinking-exp",
                  "gemini-2.0-flash-lite-preview-02-05",
                  "gemini-2.0-flash-lite-preview",
                  "gemini-2.0-flash-lite-001",
                  "gemini-2.0-flash-lite",
                  "gemini-2.0-flash-exp",
                  "gemini-2.0-flash-001",
                  "gemini-2.0-flash",
                  "gemini-1.5-pro-latest",
                  "gemini-1.5-pro-002",
                  "gemini-1.5-pro",
                  "gemini-1.5-flash-latest",
                  "gemini-1.5-flash-8b-latest",
                  "gemini-1.5-flash-8b-001",
                  "gemini-1.5-flash-8b",
                  "gemini-1.5-flash-002",
                  "gemini-1.5-flash",
                  "gemini-1.0-pro-vision-latest"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gemini-2.5-flash-lite-preview-06-17",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-Y5uD2"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "EnvoyerDonneesVerreTableComponent-D5zFJ",
        "type": "genericNode",
        "position": {
          "x": 7589.393838071045,
          "y": 3346.1530126999505
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import Output, MessageTextInput\nfrom langflow.schema import Data\nimport requests\nimport ast  # Pour convertir les chaînes de listes en vraies listes Python\nimport re\n\nclass EnvoyerDonneesVerreTableComponent(Component):\n    display_name = \"Envoyer Données Verre à la Table\"\n    description = \"Envoyer la composition détaillée du verre et les informations de référence du document au serveur Flask.\"\n    icon = \"table\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"texte_extrait\",\n            display_name=\"Texte Extrait\",\n            info=(\n                \"Texte extrait contenant la référence du document et les informations sur la composition du verre.\"\n            ),\n            value=(\n                \"1. Type du document : Article scientifique\\n\"\n                \"2. Titre du document : Can a simple topological-constraints-based model predict the initial dissolution rate of borosilicate and aluminosilicate glasses?\\n\"\n                \"3. Référence : npj Materials Degradation (2020) 4:6 ; https://doi.org/10.1038/s41529-020-0111-4\\n\"\n                \"4. Premier Auteur : Stéphane Gin\\n\"\n                \"5. Nombre de types de verres : 16\\n\"\n                \"6. Verre_type1 : NBS12/28\\n\"\n                \"7. Nombre de tests(Verre_type1) : 2\\n\"\n                \"8. Li(Verre_type1) : [0, 0]\\n\"\n                \"9. B(Verre_type1) : [39.83, 39.83]\\n\"\n                \"10. O(Verre_type1) : [0, 0]\\n\"\n                \"11. Na(Verre_type1) : [16.68, 16.68]\\n\"\n                \"12. Mg(Verre_type1) : [0, 0]\\n\"\n                \"13. Al(Verre_type1) : [0, 0]\\n\"\n                \"14. Si(Verre_type1) : [43.49, 43.49]\\n\"\n                \"15. P(Verre_type1) : [0, 0]\\n\"\n                \"16. K(Verre_type1) : [0, 0]\\n\"\n                \"17. Ca(Verre_type1) : [0, 0]\\n\"\n                \"18. Ti(Verre_type1) : [0, 0]\\n\"\n                \"19. V(Verre_type1) : [0, 0]\\n\"\n                \"20. Cr(Verre_type1) : [0, 0]\\n\"\n                \"21. Mn(Verre_type1) : [0, 0]\\n\"\n                \"22. Fe(Verre_type1) : [0, 0]\\n\"\n                \"23. Co(Verre_type1) : [0, 0]\\n\"\n                \"24. Ni(Verre_type1) : [0, 0]\\n\"\n                \"25. Cu(Verre_type1) : [0, 0]\\n\"\n                \"26. Zn(Verre_type1) : [0, 0]\\n\"\n                \"27. Ga(Verre_type1) : [0, 0]\\n\"\n                \"28. Ge(Verre_type1) : [0, 0]\\n\"\n                \"29. As(Verre_type1) : [0, 0]\\n\"\n                \"30. Se(Verre_type1) : [0, 0]\\n\"\n                \"31. Rb(Verre_type1) : [0, 0]\\n\"\n                \"32. Sr(Verre_type1) : [0, 0]\\n\"\n                \"33. Y(Verre_type1) : [0, 0]\\n\"\n                \"34. Zr(Verre_type1) : [0, 0]\\n\"\n                \"35. Nb(Verre_type1) : [0, 0]\\n\"\n                \"36. Mo(Verre_type1) : [0, 0]\\n\"\n                \"37. Tc(Verre_type1) : [0, 0]\\n\"\n                \"38. Ru(Verre_type1) : [0, 0]\\n\"\n                \"39. Rh(Verre_type1) : [0, 0]\\n\"\n                \"40. Pd(Verre_type1) : [0, 0]\\n\"\n                \"41. Ag(Verre_type1) : [0, 0]\\n\"\n                \"42. Cd(Verre_type1) : [0, 0]\\n\"\n                \"43. In(Verre_type1) : [0, 0]\\n\"\n                \"44. Sn(Verre_type1) : [0, 0]\\n\"\n                \"45. Sb(Verre_type1) : [0, 0]\\n\"\n                \"46. Te(Verre_type1) : [0, 0]\\n\"\n                \"47. I(Verre_type1) : [0, 0]\\n\"\n                \"48. Cs(Verre_type1) : [0, 0]\\n\"\n                \"49. Ba(Verre_type1) : [0, 0]\\n\"\n                \"50. La(Verre_type1) : [0, 0]\\n\"\n                \"51. Hf(Verre_type1) : [0, 0]\\n\"\n                \"52. Ta(Verre_type1) : [0, 0]\\n\"\n                \"53. W(Verre_type1) : [0, 0]\\n\"\n                \"54. Re(Verre_type1) : [0, 0]\\n\"\n                \"55. Os(Verre_type1) : [0, 0]\\n\"\n                \"56. Ir(Verre_type1) : [0, 0]\\n\"\n                \"57. Pt(Verre_type1) : [0, 0]\\n\"\n                \"58. Au(Verre_type1) : [0, 0]\\n\"\n                \"59. Hg(Verre_type1) : [0, 0]\\n\"\n                \"60. Tl(Verre_type1) : [0, 0]\\n\"\n                \"61. Pb(Verre_type1) : [0, 0]\\n\"\n                \"62. Bi(Verre_type1) : [0, 0]\\n\"\n                \"63. Po(Verre_type1) : [0, 0]\\n\"\n                \"64. At(Verre_type1) : [0, 0]\\n\"\n                \"65. Rn(Verre_type1) : [0, 0]\\n\"\n                \"66. Ce(Verre_type1) : [0, 0]\\n\"\n                \"67. Pr(Verre_type1) : [0, 0]\\n\"\n                \"68. Nd(Verre_type1) : [0, 0]\\n\"\n                \"69. S_autres_TR(Verre_type1) : [0, 0]\\n\"\n                \"70. Th(Verre_type1) : [0, 0]\\n\"\n                \"71. U(Verre_type1) : [0, 0]\\n\"\n                \"72. Pu(Verre_type1) : [0, 0]\\n\"\n                \"73. Np(Verre_type1) : [0, 0]\\n\"\n                \"74. Am(Verre_type1) : [0, 0]\\n\"\n                \"75. Cm(Verre_type1) : [0, 0]\\n\"\n                \"76. S_autres_An(Verre_type1) : [0, 0]\\n\"\n                \"77. Somme(Verre_type1) : [100.0, 100.0]\\n\"\n                \"78. Densité(Verre_type1) : [2.462, 2.462]\\n\"\n                \"79. Homogénéité(Verre_type1) : [Not available, Not available]\\n\"\n                \"80. % B(IV)(Verre_type1) : [43, 43]\\n\"\n                \"81. Irradié(Verre_type1) : [Not available, Not available]\\n\"\n                \"82. Caractéristiques si irradié(Verre_type1) : [Not available, Not available]\\n\"\n                \"83. Température(Verre_type1) : [90, 90]\\n\"\n                \"84. Statique/dynamique(Verre_type1) : [static, static]\\n\"\n                \"85. Plage granulométrique (si poudre)(Verre_type1) : [Not available, 40 –63]\\n\"\n                \"86. Surface spécifique géométrique (si poudre)(Verre_type1) : [Not available, 37.6]\\n\"\n                \"87. Surface spécifique BET (si poudre)(Verre_type1) : [Not available, Not available]\\n\"\n                \"88. Qualité de polissage (si monolithe)(Verre_type1) : [Not available, Not available]\\n\"\n                \"89. Masse du verre(Verre_type1) : [1.292, 0.080]\\n\"\n                \"90. Surface du verre (S)(Verre_type1) : [5.08, Not available]\\n\"\n                \"91. Volume de la solution (V)(Verre_type1) : [0.485, 0.999]\\n\"\n                \"92. Débit de la solution(Verre_type1) : [Not available, Not available]\\n\"\n                \"93. pH initial (T amb)(Verre_type1) : [Not available, Not available]\\n\"\n                \"94. pH initial (T essai)(Verre_type1) : [9, 9]\\n\"\n                \"95. Composition de la solution(Verre_type1) : [Not available, Not available]\\n\"\n                \"96. Durée de l'expérience(Verre_type1) : [2.4, 0.7]\\n\"\n                \"97. pH final (T amb)(Verre_type1) : [Not available, Not available]\\n\"\n                \"98. pH final (T essai)(Verre_type1) : [8.9, 9.0]\\n\"\n                \"99. Normalisation de la vitesse (Sgeo ou SBET)(Verre_type1) : [Not available, Not available]\\n\"\n                \"100. V₀(Si) ou r₀(Si)(Verre_type1) : [192, 202]\\n\"\n                \"101. r²(Si)(Verre_type1) : [1.000, 0.995]\\n\"\n                \"102. Ordonnée à l'origine (Si)(Verre_type1) : [0.4, 0.6]\\n\"\n                \"103. V₀(B) ou r₀(B)(Verre_type1) : [Not available, Not available]\\n\"\n                \"104. Ordonnée à l'origine (B)(Verre_type1) : [Not available, Not available]\\n\"\n                \"105. V₀(Na) ou r₀(Na)(Verre_type1) : [Not available, Not available]\\n\"\n                \"106. r²(Na)(Verre_type1) : [Not available, Not available]\\n\"\n                \"107. Ordonnée à l'origine (Na)(Verre_type1) : [Not available, Not available]\\n\"\n                \"108. V₀(ΔM) ou r₀(ΔM)(Verre_type1) : [223, Not available]\\n\"\n                \"109. Congruence(Verre_type1) : [1.1 ± 0.1, Not available]\\n\"\n                \"110. Verre_type2 : NBS36/21\\n\"\n                \"111. Nombre de tests(Verre_type2) : 1\\n\"\n                \"112. Li(Verre_type2) : [0]\\n\"\n                \"113. B(Verre_type2) : [26.37]\\n\"\n                \"114. O(Verre_type2) : [0]\\n\"\n                \"115. Na(Verre_type2) : [46.11]\\n\"\n                \"116. Mg(Verre_type2) : [0]\\n\"\n                \"117. Al(Verre_type2) : [0]\\n\"\n                \"118. Si(Verre_type2) : [27.52]\\n\"\n                \"119. P(Verre_type2) : [0]\\n\"\n                \"120. K(Verre_type2) : [0]\\n\"\n                \"121. Ca(Verre_type2) : [0]\\n\"\n                \"122. Ti(Verre_type2) : [0]\\n\"\n                \"123. V(Verre_type2) : [0]\\n\"\n                \"124. Cr(Verre_type2) : [0]\\n\"\n                \"125. Mn(Verre_type2) : [0]\\n\"\n                \"126. Fe(Verre_type2) : [0]\\n\"\n                \"127. Co(Verre_type2) : [0]\\n\"\n                \"128. Ni(Verre_type2) : [0]\\n\"\n                \"129. Cu(Verre_type2) : [0]\\n\"\n                \"130. Zn(Verre_type2) : [0]\\n\"\n                \"131. Ga(Verre_type2) : [0]\\n\"\n                \"132. Ge(Verre_type2) : [0]\\n\"\n                \"133. As(Verre_type2) : [0]\\n\"\n                \"134. Se(Verre_type2) : [0]\\n\"\n                \"135. Rb(Verre_type2) : [0]\\n\"\n                \"136. Sr(Verre_type2) : [0]\\n\"\n                \"137. Y(Verre_type2) : [0]\\n\"\n                \"138. Zr(Verre_type2) : [0]\\n\"\n                \"139. Nb(Verre_type2) : [0]\\n\"\n                \"140. Mo(Verre_type2) : [0]\\n\"\n                \"141. Tc(Verre_type2) : [0]\\n\"\n                \"142. Ru(Verre_type2) : [0]\\n\"\n                \"143. Rh(Verre_type2) : [0]\\n\"\n                \"144. Pd(Verre_type2) : [0]\\n\"\n                \"145. Ag(Verre_type2) : [0]\\n\"\n                \"146. Cd(Verre_type2) : [0]\\n\"\n                \"147. In(Verre_type2) : [0]\\n\"\n                \"148. Sn(Verre_type2) : [0]\\n\"\n                \"149. Sb(Verre_type2) : [0]\\n\"\n                \"150. Te(Verre_type2) : [0]\\n\"\n                \"151. I(Verre_type2) : [0]\\n\"\n                \"152. Cs(Verre_type2) : [0]\\n\"\n                \"153. Ba(Verre_type2) : [0]\\n\"\n                \"154. La(Verre_type2) : [0]\\n\"\n                \"155. Hf(Verre_type2) : [0]\\n\"\n                \"156. Ta(Verre_type2) : [0]\\n\"\n                \"157. W(Verre_type2) : [0]\\n\"\n                \"158. Re(Verre_type2) : [0]\\n\"\n                \"159. Os(Verre_type2) : [0]\\n\"\n                \"160. Ir(Verre_type2) : [0]\\n\"\n                \"161. Pt(Verre_type2) : [0]\\n\"\n                \"162. Au(Verre_type2) : [0]\\n\"\n                \"163. Hg(Verre_type2) : [0]\\n\"\n                \"164. Tl(Verre_type2) : [0]\\n\"\n                \"165. Pb(Verre_type2) : [0]\\n\"\n                \"166. Bi(Verre_type2) : [0]\\n\"\n                \"167. Po(Verre_type2) : [0]\\n\"\n                \"168. At(Verre_type2) : [0]\\n\"\n                \"169. Rn(Verre_type2) : [0]\\n\"\n                \"170. Ce(Verre_type2) : [0]\\n\"\n                \"171. Pr(Verre_type2) : [0]\\n\"\n                \"172. Nd(Verre_type2) : [0]\\n\"\n                \"173. S_autres_TR(Verre_type2) : [0]\\n\"\n                \"174. Th(Verre_type2) : [0]\\n\"\n                \"175. U(Verre_type2) : [0]\\n\"\n                \"176. Pu(Verre_type2) : [0]\\n\"\n                \"177. Np(Verre_type2) : [0]\\n\"\n                \"178. Am(Verre_type2) : [0]\\n\"\n                \"179. Cm(Verre_type2) : [0]\\n\"\n                \"180. S_autres_An(Verre_type2) : [0]\\n\"\n                \"181. Somme(Verre_type2) : [100.0]\\n\"\n                \"182. Densité(Verre_type2) : [2.537]\\n\"\n                \"183. Homogénéité(Verre_type2) : [Not available]\\n\"\n                \"184. % B(IV)(Verre_type2) : [63]\\n\"\n                \"185. Irradié(Verre_type2) : [Not available]\\n\"\n                \"186. Caractéristiques si irradié(Verre_type2) : [Not available]\\n\"\n                \"187. Température(Verre_type2) : [90]\\n\"\n                \"188. Statique/dynamique(Verre_type2) : [static]\\n\"\n                \"189. Plage granulométrique (si poudre)(Verre_type2) : [Not available]\\n\"\n                \"190. Surface spécifique géométrique (si poudre)(Verre_type2) : [Not available]\\n\"\n                \"191. Surface spécifique BET (si poudre)(Verre_type2) : [Not available]\\n\"\n                \"192. Qualité de polissage (si monolithe)(Verre_type2) : [Not available]\\n\"\n                \"193. Masse du verre(Verre_type2) : [1.986]\\n\"\n                \"194. Surface du verre (S)(Verre_type2) : [6.18]\\n\"\n                \"195. Volume de la solution (V)(Verre_type2) : [1.017]\\n\"\n                \"196. Débit de la solution(Verre_type2) : [Not available]\\n\"\n                \"197. pH initial (T amb)(Verre_type2) : [Not available]\\n\"\n                \"198. pH initial (T essai)(Verre_type2) : [9]\\n\"\n                \"199. Composition de la solution(Verre_type2) : [Not available]\\n\"\n                \"200. Durée de l'expérience(Verre_type2) : [0.4]\\n\"\n                \"201. pH final (T amb)(Verre_type2) : [Not available]\\n\"\n                \"202. pH final (T essai)(Verre_type2) : [9.5]\\n\"\n                \"203. Normalisation de la vitesse (Sgeo ou SBET)(Verre_type2) : [Not available]\\n\"\n                \"204. V₀(Si) ou r₀(Si)(Verre_type2) : [47370]\\n\"\n                \"205. r²(Si)(Verre_type2) : [0.986]\\n\"\n                \"206. Ordonnée à l'origine (Si)(Verre_type2) : [-31.2]\\n\"\n                \"207. V₀(B) ou r₀(B)(Verre_type2) : [Not available]\\n\"\n                \"208. Ordonnée à l'origine (B)(Verre_type2) : [Not available]\\n\"\n                \"209. V₀(Na) ou r₀(Na)(Verre_type2) : [Not available]\\n\"\n                \"210. r²(Na)(Verre_type2) : [Not available]\\n\"\n                \"211. Ordonnée à l'origine (Na)(Verre_type2) : [Not available]\\n\"\n                \"212. V₀(ΔM) ou r₀(ΔM)(Verre_type2) : [Not available]\\n\"\n                \"213. Congruence(Verre_type2) : [1.1 ± 0.1]\\n\"\n            ),\n            tool_mode=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Réponse\", name=\"sortie\", method=\"construire_sortie\"),\n    ]\n\n    def construire_sortie(self) -> Data:\n        texte_extrait = self.texte_extrait\n        print(f\"Texte Extrait: {texte_extrait}\")\n    \n        try:\n            # Nettoyer et analyser le texte\n            lignes = [ligne.strip() for ligne in texte_extrait.split(\"\\n\") if ligne.strip()]\n    \n            # Extraction des données générales\n            type_doc = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"1. Type du document :\")), None)\n            titre = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"2. Titre du document :\")), None)\n            reference = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"3. Référence :\")), None)\n            premier_auteur = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"4. Premier Auteur :\")), None)\n            nombre_types_verres_str = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(\"5. Nombre de types de verres :\")), None)\n            print(f\"Nombre de types de verres (str): {nombre_types_verres_str}\")\n    \n            # Convertir en entier avec vérification\n            if nombre_types_verres_str is not None:\n                nombre_types_verres = int(nombre_types_verres_str)\n            else:\n                raise ValueError(\"Le nombre de types de verres n'a pas été trouvé dans le texte.\")\n    \n            # Liste pour stocker tous les verres (une entrée par test)\n            donnees_verres = []\n    \n            # Liste complète des paramètres à extraire, correspondant exactement au Flask\n            parametres = [\n                \"Li\", \"B\", \"O\", \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"K\", \"Ca\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\", \"Ga\", \"Ge\", \"As\", \"Se\", \"Rb\", \"Sr\", \"Y\", \"Zr\", \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\", \"Sb\", \"Te\", \"I\", \"Cs\", \"Ba\", \"La\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\", \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Ce\", \"Pr\", \"Nd\", \"S_autres_TR\", \"Th\", \"U\", \"Pu\", \"Np\", \"Am\", \"Cm\", \"S_autres_An\", \"Somme\", \"Densité\", \"Homogénéité\", \"% B(IV)\", \"Irradié\", \"Caractéristiques si irradié\", \"Température\", \"Statique/dynamique\", \"Plage granulométrique (si poudre)\", \"Surface spécifique géométrique (si poudre)\", \"Surface spécifique BET (si poudre)\", \"Qualité de polissage (si monolithe)\", \"Masse du verre\", \"Surface du_verre (S)\", \"Volume de la solution (V)\", \"Débit de la solution\", \"pH initial (T amb)\", \"pH initial (T essai)\", \"Composition de la solution\", \"Durée de l'expérience\", \"pH final (T amb)\", \"pH final (T essai)\", \"Normalisation de la vitesse (Sgeo ou SBET)\", \"V₀(Si) ou r₀(Si)\", \"r²(Si)\", \"Ordonnée à l'origine (Si)\", \"V₀(B) ou r₀(B)\", \"Ordonnée à l'origine (B)\", \"V₀(Na) ou r₀(Na)\", \"r²(Na)\", \"Ordonnée à l'origine (Na)\", \"V₀(ΔM) ou r₀(ΔM)\", \"Congruence\"\n            ]\n    \n            # Dictionnaire de correspondance entre paramètres d'entrée et clés Flask\n            parametres_flask = {\n                \"Li\": \"Li\",\n                \"B\": \"B\",\n                \"O\": \"O\",\n                \"Na\": \"Na\",\n                \"Mg\": \"Mg\",\n                \"Al\": \"Al\",\n                \"Si\": \"Si\",\n                \"P\": \"P\",\n                \"K\": \"K\",\n                \"Ca\": \"Ca\",\n                \"Ti\": \"Ti\",\n                \"V\": \"V\",\n                \"Cr\": \"Cr\",\n                \"Mn\": \"Mn\",\n                \"Fe\": \"Fe\",\n                \"Co\": \"Co\",\n                \"Ni\": \"Ni\",\n                \"Cu\": \"Cu\",\n                \"Zn\": \"Zn\",\n                \"Ga\": \"Ga\",\n                \"Ge\": \"Ge\",\n                \"As\": \"As\",\n                \"Se\": \"Se\",\n                \"Rb\": \"Rb\",\n                \"Sr\": \"Sr\",\n                \"Y\": \"Y\",\n                \"Zr\": \"Zr\",\n                \"Nb\": \"Nb\",\n                \"Mo\": \"Mo\",\n                \"Tc\": \"Tc\",\n                \"Ru\": \"Ru\",\n                \"Rh\": \"Rh\",\n                \"Pd\": \"Pd\",\n                \"Ag\": \"Ag\",\n                \"Cd\": \"Cd\",\n                \"In\": \"In\",\n                \"Sn\": \"Sn\",\n                \"Sb\": \"Sb\",\n                \"Te\": \"Te\",\n                \"I\": \"I\",\n                \"Cs\": \"Cs\",\n                \"Ba\": \"Ba\",\n                \"La\": \"La\",\n                \"Hf\": \"Hf\",\n                \"Ta\": \"Ta\",\n                \"W\": \"W\",\n                \"Re\": \"Re\",\n                \"Os\": \"Os\",\n                \"Ir\": \"Ir\",\n                \"Pt\": \"Pt\",\n                \"Au\": \"Au\",\n                \"Hg\": \"Hg\",\n                \"Tl\": \"Tl\",\n                \"Pb\": \"Pb\",\n                \"Bi\": \"Bi\",\n                \"Po\": \"Po\",\n                \"At\": \"At\",\n                \"Rn\": \"Rn\",\n                \"Ce\": \"Ce\",\n                \"Pr\": \"Pr\",\n                \"Nd\": \"Nd\",\n                \"S_autres_TR\": \"S_autres_TR\",\n                \"Th\": \"Th\",\n                \"U\": \"U\",\n                \"Pu\": \"Pu\",\n                \"Np\": \"Np\",\n                \"Am\": \"Am\",\n                \"Cm\": \"Cm\",\n                \"S_autres_An\": \"S_autres_An\",\n                \"Somme\": \"Somme\",\n                \"Densité\": \"Densité\",\n                \"Homogénéité\": \"Homogénéité\",\n                \"% B(IV)\": \"B_IV\",\n                \"Irradié\": \"Irradié\",\n                \"Caractéristiques si irradié\": \"Caractéristiques_si_irradié\",\n                \"Température\": \"Température\",\n                \"Statique/dynamique\": \"Statique_dynamique\",\n                \"Plage granulométrique (si poudre)\": \"Plage_granulométrique_si_poudre\",\n                \"Surface spécifique géométrique (si poudre)\": \"Surface_spécifique_géométrique_si_poudre\",\n                \"Surface spécifique BET (si poudre)\": \"Surface_spécifique_BET_si_poudre\",\n                \"Qualité de polissage (si monolithe)\": \"Qualité_de_polissage_si_monolithe\",\n                \"Masse du verre\": \"Masse_du_verre\",\n                \"Surface du_verre (S)\": \"Surface_du_verre_S\",\n                \"Volume de la solution (V)\": \"Volume_de_la_solution_V\",\n                \"Débit de la solution\": \"Débit_de_la_solution\",\n                \"pH initial (T amb)\": \"pH_initial_T_amb\",\n                \"pH initial (T essai)\": \"pH_initial_T_essai\",\n                \"Composition de la solution\": \"Composition_de_la_solution\",\n                \"Durée de l'expérience\": \"Durée_de_l_expérience\",\n                \"pH final (T amb)\": \"pH_final_T_amb\",\n                \"pH final (T essai)\": \"pH_final_T_essai\",\n                \"Normalisation de la vitesse (Sgeo ou SBET)\": \"Normalisation_de_la_vitesse_Sgeo_ou_SBET\",\n                \"V₀(Si) ou r₀(Si)\": \"V₀(Si) ou r₀(Si)\",\n                \"r²(Si)\": \"r²(Si)\",\n                \"Ordonnée à l'origine (Si)\": \"Ordonnée_à_l'origine_Si\",\n                \"V₀(B) ou r₀(B)\": \"V₀(B) ou r₀(B)\",\n                \"Ordonnée à l'origine (B)\": \"Ordonnée_à_l'origine_B\",\n                \"V₀(Na) ou r₀(Na)\": \"V₀(Na) ou r₀(Na)\",\n                \"r²(Na)\": \"r²(Na)\",\n                \"Ordonnée à l'origine (Na)\": \"Ordonnée_à_l'origine_Na\",\n                \"V₀(ΔM) ou r₀(ΔM)\": \"V₀(ΔM) ou r₀(ΔM)\",\n                \"Congruence\": \"Congruence\"\n            }\n    \n            for i in range(nombre_types_verres):\n                verre_type_key = f\"Verre_type{i+1}\"\n                type_verre = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{6 + i * 104}. {verre_type_key} :\")), None)\n                print(f\"Type de verre {verre_type_key}: {type_verre}\")\n    \n                # Extraction du nombre de tests avec gestion de None\n                nombre_tests_str = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{7 + i * 104}. Nombre de tests({verre_type_key}) :\")), None)\n                print(f\"Nombre de tests (str) pour {verre_type_key}: {nombre_tests_str}\")\n                if nombre_tests_str is not None:\n                    nombre_tests = int(nombre_tests_str)\n                else:\n                    nombre_tests = 1\n    \n                # Extraction des listes pour chaque paramètre avec gestion des erreurs\n                params = {}\n                for idx, param in enumerate(parametres):\n                    # Normaliser le nom du paramètre pour gérer les variantes\n                    param_key = param.replace(\"_\", \" \").replace(\"S autres TR\", \"S(autres TR)\").replace(\"S autres An\", \"S(autres An)\")\n                    param_value_str = next((ligne.split(\":\", 1)[1].strip() for ligne in lignes if ligne.startswith(f\"{8 + i * 104 + idx}. {param_key}({verre_type_key}) :\")), None)\n                    print(f\"Valeur extraite pour {param}({verre_type_key}): {param_value_str}\")\n                    if param_value_str:\n                        # Nettoyer la chaîne\n                        param_value_str = re.sub(r'–', '-', param_value_str)  # Remplacer les tirets par des signes moins\n                        param_value_str = re.sub(r'\\s+', ' ', param_value_str)  # Réduire les espaces multiples\n                        try:\n                            # Essayer de parser avec ast.literal_eval\n                            params[param] = ast.literal_eval(param_value_str)\n                        except (ValueError, SyntaxError):\n                            # Si le parsing échoue, traiter comme une liste de chaînes\n                            items = [item.strip() for item in param_value_str.strip(\"[]\").split(\",\")]\n                            cleaned_items = []\n                            for item in items:\n                                try:\n                                    cleaned_items.append(float(item))  # Si c’est un nombre, le convertir\n                                except ValueError:\n                                    cleaned_items.append(item)  # Sinon, le laisser comme chaîne\n                            params[param] = cleaned_items\n                    else:\n                        params[param] = None\n    \n                # Créer une entrée pour chaque test\n                for j in range(nombre_tests):\n                    verre_data = {\n                        \"type\": f\"{type_verre}_test{j+1}\" if nombre_tests > 1 else type_verre,  # Nom unique pour chaque test\n                    }\n                    for param, flask_key in parametres_flask.items():\n                        if params[param] and len(params[param]) > j:\n                            verre_data[flask_key] = params[param][j]\n                        else:\n                            verre_data[flask_key] = \"Not available\"\n                    donnees_verres.append(verre_data)\n    \n            # Préparer les données à envoyer\n            url = 'http://127.0.0.1:5002/add_glass_data'\n            donnees = {\n                \"type\": type_doc,\n                \"titre\": titre,\n                \"reference\": reference,\n                \"premier_auteur\": premier_auteur,\n                \"nombre_types_verres\": len(donnees_verres),  # Nombre total d’entrées (une par test)\n                \"verres\": donnees_verres\n            }\n            print(f\"Envoi des données: {donnees}\")\n    \n            reponse = requests.post(url, json=donnees)\n    \n            if reponse.status_code == 200:\n                return Data(value=\"Données du verre ajoutées avec succès!\")\n            else:\n                return Data(value=f\"Erreur lors de l'ajout des données du verre. Code d'état: {reponse.status_code} - {reponse.text}\")\n    \n        except Exception as e:\n            return Data(value=f\"Exception survenue: {str(e)}\")\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "texte_extrait": {
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "texte_extrait",
                "value": "",
                "display_name": "Texte Extrait",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Texte extrait contenant la référence du document et les informations sur la composition du verre.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Envoyer la composition détaillée du verre et les informations de référence du document au serveur Flask.",
            "icon": "table",
            "base_classes": [
              "Data"
            ],
            "display_name": "Envoyer Données Verre à la Table",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "sortie",
                "hidden": null,
                "display_name": "Réponse",
                "method": "construire_sortie",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "texte_extrait"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false
          },
          "showNode": true,
          "type": "EnvoyerDonneesVerreTableComponent",
          "id": "EnvoyerDonneesVerreTableComponent-D5zFJ"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 271
        },
        "dragging": false
      },
      {
        "id": "GoogleGenerativeAIModel-fa2Vp",
        "type": "genericNode",
        "position": {
          "x": 4433.55202786732,
          "y": 45
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "real_time_refresh": true,
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport requests\nfrom loguru import logger\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import DropdownInput, FloatInput, IntInput, SecretStrInput, SliderInput\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.schema import dotdict\n\n\nclass GoogleGenerativeAIComponent(LCModelComponent):\n    display_name = \"LiteLLM IXIA V0.3\"\n    description = \"Connector to LLM via LiteLLM API on IXIA\"\n    icon = \"🚄\"\n    name = \"LiteLLM IXIA V0.3\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_output_tokens\", display_name=\"Max Output Tokens\", info=\"The maximum number of tokens to generate.\"\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model\",\n            info=\"The name of the model to use.\",\n            options=GOOGLE_GENERATIVE_AI_MODELS,\n            value=\"gemini-1.5-pro\",\n            refresh_button=True,\n            combobox=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Google API Key\",\n            info=\"The Google API Key to use for the Google Generative AI.\",\n            required=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. \"\n            \"Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"tool_model_enabled\",\n            display_name=\"Tool Model Enabled\",\n            info=\"Whether to use the tool model.\",\n            value=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        try:\n            from langchain_google_genai import ChatGoogleGenerativeAI\n        except ImportError as e:\n            msg = \"The 'langchain_google_genai' package is required to use the Google Generative AI model.\"\n            raise ImportError(msg) from e\n\n        google_api_key = self.api_key\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        temperature = self.temperature\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key).get_secret_value(),\n        )\n\n    def get_models(self, tool_model_enabled: bool | None = None) -> list[str]:\n        try:\n            import google.generativeai as genai\n\n            genai.configure(api_key=self.api_key)\n            model_ids = [\n                model.name.replace(\"models/\", \"\")\n                for model in genai.list_models()\n                if \"generateContent\" in model.supported_generation_methods\n            ]\n            model_ids.sort(reverse=True)\n        except (ImportError, ValueError) as e:\n            logger.exception(f\"Error getting model names: {e}\")\n            model_ids = GOOGLE_GENERATIVE_AI_MODELS\n        if tool_model_enabled:\n            try:\n                from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n            except ImportError as e:\n                msg = \"langchain_google_genai is not installed.\"\n                raise ImportError(msg) from e\n            for model in model_ids:\n                model_with_tool = ChatGoogleGenerativeAI(\n                    model=self.model_name,\n                    google_api_key=self.api_key,\n                )\n                if not self.supports_tool_calling(model_with_tool):\n                    model_ids.remove(model)\n        return model_ids\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name in (\"base_url\", \"model_name\", \"tool_model_enabled\", \"api_key\") and field_value:\n            try:\n                if len(self.api_key) == 0:\n                    ids = GOOGLE_GENERATIVE_AI_MODELS\n                else:\n                    try:\n                        ids = self.get_models(tool_model_enabled=self.tool_model_enabled)\n                    except (ImportError, ValueError, requests.exceptions.RequestException) as e:\n                        logger.exception(f\"Error getting model names: {e}\")\n                        ids = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"options\"] = ids\n                build_config[\"model_name\"][\"value\"] = ids[0]\n            except Exception as e:\n                msg = f\"Error getting model names: {e}\"\n                raise ValueError(msg) from e\n        return build_config\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "tool_mode": false,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 10000,
                "display_name": "Max Output Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "gemini-1.5-pro",
                  "gemini-1.5-flash",
                  "gemini-1.5-flash-8b",
                  "gemini-2.0-flash",
                  "gemini-exp-1206",
                  "gemini-2.0-flash-thinking-exp-01-21",
                  "learnlm-1.5-pro-experimental",
                  "gemma-2-2b",
                  "gemma-2-9b",
                  "gemma-2-27b"
                ],
                "options_metadata": [],
                "combobox": true,
                "dialog_inputs": {},
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "learnlm-2.0-flash-experimental",
                "display_name": "Model",
                "advanced": true,
                "dynamic": false,
                "info": "The name of the model to use.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput",
                "load_from_db": false
              },
              "n": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "temperature": {
                "tool_mode": false,
                "min_label": "",
                "max_label": "",
                "min_label_icon": "",
                "max_label_icon": "",
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "range_spec": {
                  "step_type": "float",
                  "min": 0,
                  "max": 2,
                  "step": 0.01
                },
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "title_case": false,
                "type": "slider",
                "_input_type": "SliderInput",
                "load_from_db": false
              },
              "tool_model_enabled": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tool_model_enabled",
                "value": false,
                "display_name": "Tool Model Enabled",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to use the tool model.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "top_k": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Connector to LLM via LiteLLM API on IXIA",
            "icon": "🚄",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "LiteLLM IXIA V0.3",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Message",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "api_key"
                ],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_output_tokens",
              "model_name",
              "api_key",
              "top_p",
              "temperature",
              "n",
              "top_k",
              "tool_model_enabled"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "type": "LiteLLM IXIA V0.3",
          "id": "GoogleGenerativeAIModel-fa2Vp"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 446
        }
      },
      {
        "id": "CombineText-JV5OM",
        "type": "genericNode",
        "position": {
          "x": 7063.672003017616,
          "y": 2726.503584973799
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass CombineTextComponent(Component):\n    display_name = \"Combine Text\"\n    description = \"Concatenate multiple text sources into a single text chunk using a specified delimiter.\"\n    icon = \"merge\"\n    name = \"CombineText\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text1\",\n            display_name=\"First Text\",\n            info=\"The first text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text2\",\n            display_name=\"Second Text\",\n            info=\"The second text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text3\",\n            display_name=\"Third Text\",\n            info=\"The third text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text4\",\n            display_name=\"Fourth Text\",\n            info=\"The fourth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text5\",\n            display_name=\"Fifth Text\",\n            info=\"The fifth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text6\",\n            display_name=\"Sixth Text\",\n            info=\"The sixth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text7\",\n            display_name=\"Seventh Text\",\n            info=\"The seventh text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text8\",\n            display_name=\"Eighth Text\",\n            info=\"The eighth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text9\",\n            display_name=\"Ninth Text\",\n            info=\"The ninth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text10\",\n            display_name=\"Tenth Text\",\n            info=\"The tenth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text11\",\n            display_name=\"Eleventh Text\",\n            info=\"The eleventh text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text12\",\n            display_name=\"Twelfth Text\",\n            info=\"The twelfth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text13\",\n            display_name=\"Thirteenth Text\",\n            info=\"The thirteenth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text14\",\n            display_name=\"Fourteenth Text\",\n            info=\"The fourteenth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text15\",\n            display_name=\"Fifteenth Text\",\n            info=\"The fifteenth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"text16\",\n            display_name=\"Sixteenth Text\",\n            info=\"The sixteenth text input to concatenate.\",\n        ),\n        MessageTextInput(\n            name=\"delimiter\",\n            display_name=\"Delimiter\",\n            info=\"A string used to separate the text inputs. Defaults to a whitespace.\",\n            value=\"\\n\",  # Utilisez un retour à la ligne comme délimiteur par défaut\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Combined Text\", name=\"combined_text\", method=\"combine_texts\"),\n    ]\n\n    def combine_texts(self) -> Message:\n        # Récupérer les valeurs des 16 entrées de texte\n        texts = [getattr(self, f\"text{i}\") for i in range(1, 17)]\n\n        # Ajouter des impressions pour le débogage\n        for i, text in enumerate(texts, start=1):\n            print(f\"Text {i} length: {len(text)}\")\n\n        # Utiliser le délimiteur spécifié pour joindre les textes\n        combined = self.delimiter.join(texts)\n        self.status = combined\n        return Message(text=combined)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "delimiter": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delimiter",
                "value": " \\n",
                "display_name": "Delimiter",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A string used to separate the text inputs. Defaults to a whitespace.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text1": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text1",
                "value": "",
                "display_name": "First Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The first text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text10": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text10",
                "value": "",
                "display_name": "Tenth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The tenth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text11": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text11",
                "value": "",
                "display_name": "Eleventh Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The eleventh text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text12": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text12",
                "value": "",
                "display_name": "Twelfth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The twelfth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text13": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text13",
                "value": "",
                "display_name": "Thirteenth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The thirteenth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text14": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text14",
                "value": "",
                "display_name": "Fourteenth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The fourteenth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text15": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text15",
                "value": "",
                "display_name": "Fifteenth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The fifteenth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text16": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text16",
                "value": "",
                "display_name": "Sixteenth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The sixteenth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text2": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text2",
                "value": "",
                "display_name": "Second Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The second text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text3": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text3",
                "value": "",
                "display_name": "Third Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The third text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text4": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text4",
                "value": "",
                "display_name": "Fourth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The fourth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text5": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text5",
                "value": "",
                "display_name": "Fifth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The fifth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text6": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text6",
                "value": "",
                "display_name": "Sixth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The sixth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text7": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text7",
                "value": "",
                "display_name": "Seventh Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The seventh text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text8": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text8",
                "value": "",
                "display_name": "Eighth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The eighth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "text9": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text9",
                "value": "",
                "display_name": "Ninth Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The ninth text input to concatenate.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Concatenate multiple text sources into a single text chunk using a specified delimiter.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Combine Text",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_text",
                "hidden": null,
                "display_name": "Combined Text",
                "method": "combine_texts",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null,
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "text1",
              "text2",
              "text3",
              "text4",
              "text5",
              "text6",
              "text7",
              "text8",
              "text9",
              "text10",
              "text11",
              "text12",
              "text13",
              "text14",
              "text15",
              "text16",
              "delimiter"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "CombineText",
          "id": "CombineText-JV5OM"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 1577
        },
        "dragging": false
      },
      {
        "id": "File-2qZjd",
        "type": "genericNode",
        "position": {
          "x": 532.489012779848,
          "y": 3518.1559833199185
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "file_path": {
                "trace_as_metadata": true,
                "list": true,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "file_path",
                "value": "",
                "display_name": "Server File Path",
                "advanced": true,
                "input_types": [
                  "Data",
                  "Message"
                ],
                "dynamic": false,
                "info": "Data object with a 'file_path' property pointing to server file or a Message object with a path to the file. Supercedes 'Path' but supports same file types.",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "path": {
                "trace_as_metadata": true,
                "file_path": "0130294b-d3dd-4db9-84ec-bd9043e6bc1d/2025-07-15_09-57-58_Gin.pdf",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "zip",
                  "tar",
                  "tgz",
                  "bz2",
                  "gz"
                ],
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "path",
                "value": "",
                "display_name": "Path",
                "advanced": false,
                "dynamic": false,
                "info": "Supported file extensions: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx; optionally bundled in file extensions: zip, tar, tgz, bz2, gz",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.data import BaseFileComponent\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.io import BoolInput, IntInput\nfrom langflow.schema import Data\n\n\nclass FileComponent(BaseFileComponent):\n    \"\"\"Handles loading and processing of individual or zipped text files.\n\n    This component supports processing multiple valid files within a zip archive,\n    resolving paths, validating file types, and optionally using multithreading for processing.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    VALID_EXTENSIONS = TEXT_FILE_TYPES\n\n    inputs = [\n        *BaseFileComponent._base_inputs,\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"[Deprecated] Use Multithreading\",\n            advanced=True,\n            value=True,\n            info=\"Set 'Processing Concurrency' greater than 1 to enable multithreading.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Processing Concurrency\",\n            advanced=True,\n            info=\"When multiple files are being processed, the number of files to process concurrently.\",\n            value=1,\n        ),\n    ]\n\n    outputs = [\n        *BaseFileComponent._base_outputs,\n    ]\n\n    def process_files(self, file_list: list[BaseFileComponent.BaseFile]) -> list[BaseFileComponent.BaseFile]:\n        \"\"\"Processes files either sequentially or in parallel, depending on concurrency settings.\n\n        Args:\n            file_list (list[BaseFileComponent.BaseFile]): List of files to process.\n\n        Returns:\n            list[BaseFileComponent.BaseFile]: Updated list of files with merged data.\n        \"\"\"\n\n        def process_file(file_path: str, *, silent_errors: bool = False) -> Data | None:\n            \"\"\"Processes a single file and returns its Data object.\"\"\"\n            try:\n                return parse_text_file_to_data(file_path, silent_errors=silent_errors)\n            except FileNotFoundError as e:\n                msg = f\"File not found: {file_path}. Error: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n            except Exception as e:\n                msg = f\"Unexpected error processing {file_path}: {e}\"\n                self.log(msg)\n                if not silent_errors:\n                    raise\n                return None\n\n        if not file_list:\n            msg = \"No files to process.\"\n            raise ValueError(msg)\n\n        concurrency = 1 if not self.use_multithreading else max(1, self.concurrency_multithreading)\n        file_count = len(file_list)\n\n        parallel_processing_threshold = 2\n        if concurrency < parallel_processing_threshold or file_count < parallel_processing_threshold:\n            if file_count > 1:\n                self.log(f\"Processing {file_count} files sequentially.\")\n            processed_data = [process_file(str(file.path), silent_errors=self.silent_errors) for file in file_list]\n        else:\n            self.log(f\"Starting parallel processing of {file_count} files with concurrency: {concurrency}.\")\n            file_paths = [str(file.path) for file in file_list]\n            processed_data = parallel_load_data(\n                file_paths,\n                silent_errors=self.silent_errors,\n                load_function=process_file,\n                max_concurrency=concurrency,\n            )\n\n        # Use rollup_basefile_data to merge processed data with BaseFile objects\n        return self.rollup_data(file_list, processed_data)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "concurrency_multithreading": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "concurrency_multithreading",
                "value": 1,
                "display_name": "Processing Concurrency",
                "advanced": true,
                "dynamic": false,
                "info": "When multiple files are being processed, the number of files to process concurrently.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "delete_server_file_after_processing": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "delete_server_file_after_processing",
                "value": true,
                "display_name": "Delete Server File After Processing",
                "advanced": true,
                "dynamic": false,
                "info": "If true, the Server File Path will be deleted after processing.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "ignore_unspecified_files": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "ignore_unspecified_files",
                "value": false,
                "display_name": "Ignore Unspecified Files",
                "advanced": true,
                "dynamic": false,
                "info": "If true, Data with no 'file_path' property will be ignored.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "ignore_unsupported_extensions": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "ignore_unsupported_extensions",
                "value": true,
                "display_name": "Ignore Unsupported Extensions",
                "advanced": true,
                "dynamic": false,
                "info": "If true, files with unsupported extensions will not be processed.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "silent_errors": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "silent_errors",
                "value": false,
                "display_name": "Silent Errors",
                "advanced": true,
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "use_multithreading": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "list_add_label": "Add More",
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "use_multithreading",
                "value": true,
                "display_name": "[Deprecated] Use Multithreading",
                "advanced": true,
                "dynamic": false,
                "info": "Set 'Processing Concurrency' greater than 1 to enable multithreading.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Load a file to be used in your project.",
            "icon": "file-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "File",
            "documentation": "",
            "minimized": false,
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "data",
                "display_name": "Data",
                "method": "load_files",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [],
                "allows_loop": false,
                "tool_mode": true
              }
            ],
            "field_order": [
              "path",
              "file_path",
              "silent_errors",
              "delete_server_file_after_processing",
              "ignore_unsupported_extensions",
              "ignore_unspecified_files",
              "use_multithreading",
              "concurrency_multithreading"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.2.0"
          },
          "showNode": true,
          "type": "File",
          "id": "File-2qZjd"
        },
        "selected": false,
        "measured": {
          "width": 320,
          "height": 228
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-CvBi7",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-CvBi7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-CvBi7{œfieldNameœ:œDocumentœ,œidœ:œPrompt-CvBi7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-CvBi7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-Prpyc",
        "target": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-Prpycœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-Prpyc{œdataTypeœ:œPromptœ,œidœ:œPrompt-Prpycœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-uYxja{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-Prpyc",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-uYxja",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-Prpyc",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Prpycœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-Prpyc{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Prpycœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-Prpyc",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-95tm9",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-95tm9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-95tm9{œfieldNameœ:œDocumentœ,œidœ:œPrompt-95tm9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-95tm9",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-BTfkR",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-BTfkRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-BTfkR{œfieldNameœ:œDocumentœ,œidœ:œPrompt-BTfkRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-BTfkR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-nDAxX",
        "target": "CombineText-2gWvC",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-nDAxXœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-2gWvCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-nDAxX{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-nDAxXœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-2gWvC{œfieldNameœ:œtext2œ,œidœ:œCombineText-2gWvCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-nDAxX",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-2gWvC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-KHumU",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-KHumUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-KHumU{œfieldNameœ:œDocumentœ,œidœ:œPrompt-KHumUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-KHumU",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-2gWvC",
        "target": "GoogleGenerativeAIModel-FfeYQ",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-2gWvCœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-2gWvC{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-2gWvCœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-FfeYQ{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-2gWvC",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-FfeYQ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-KHumU",
        "target": "GoogleGenerativeAIModel-FfeYQ",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-KHumUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-KHumU{œdataTypeœ:œPromptœ,œidœ:œPrompt-KHumUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-FfeYQ{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-KHumU",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-FfeYQ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-CvBi7",
        "target": "GoogleGenerativeAIModel-H9Xox",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-CvBi7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-CvBi7{œdataTypeœ:œPromptœ,œidœ:œPrompt-CvBi7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-H9Xox{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-CvBi7",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-H9Xox",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-Sblyw",
        "target": "CombineText-DHXvR",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-Sblywœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-DHXvRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-Sblyw{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-Sblywœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-DHXvR{œfieldNameœ:œtext2œ,œidœ:œCombineText-DHXvRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-Sblyw",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-DHXvR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-Uyk32",
        "target": "GoogleGenerativeAIModel-yDfoE",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-Uyk32œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-Uyk32{œdataTypeœ:œPromptœ,œidœ:œPrompt-Uyk32œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-yDfoE{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-Uyk32",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-yDfoE",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-DHXvR",
        "target": "GoogleGenerativeAIModel-yDfoE",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-DHXvRœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-DHXvR{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-DHXvRœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-yDfoE{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-DHXvR",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-yDfoE",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-Uyk32",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Uyk32œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-Uyk32{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Uyk32œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-Uyk32",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-LEw60",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-LEw60œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-LEw60{œfieldNameœ:œDocumentœ,œidœ:œPrompt-LEw60œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-LEw60",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-VOPkO",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-VOPkOœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-VOPkO{œfieldNameœ:œDocumentœ,œidœ:œPrompt-VOPkOœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-VOPkO",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-0TzDg",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-0TzDgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-0TzDg{œfieldNameœ:œDocumentœ,œidœ:œPrompt-0TzDgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-0TzDg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-QYsZH",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-QYsZHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-QYsZH{œfieldNameœ:œDocumentœ,œidœ:œPrompt-QYsZHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-QYsZH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-YSKtu",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-YSKtuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-YSKtu{œfieldNameœ:œDocumentœ,œidœ:œPrompt-YSKtuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-YSKtu",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-aYNdd",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-aYNddœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-aYNdd{œfieldNameœ:œDocumentœ,œidœ:œPrompt-aYNddœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-aYNdd",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-yPJsv",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-yPJsvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-yPJsv{œfieldNameœ:œDocumentœ,œidœ:œPrompt-yPJsvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-yPJsv",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-QOkbs",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-QOkbsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-QOkbs{œfieldNameœ:œDocumentœ,œidœ:œPrompt-QOkbsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-QOkbs",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-pseV4",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-pseV4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-pseV4{œfieldNameœ:œDocumentœ,œidœ:œPrompt-pseV4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-pseV4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-Vuct4",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Vuct4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-Vuct4{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Vuct4œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-Vuct4",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-zKzbb",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-zKzbbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-zKzbb{œfieldNameœ:œDocumentœ,œidœ:œPrompt-zKzbbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-zKzbb",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-R5RgD",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-R5RgDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-R5RgD{œfieldNameœ:œDocumentœ,œidœ:œPrompt-R5RgDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-R5RgD",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-po5vR",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-po5vRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-po5vR{œfieldNameœ:œDocumentœ,œidœ:œPrompt-po5vRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-po5vR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-Bia8u",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Bia8uœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-Bia8u{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Bia8uœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-Bia8u",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-KDx27",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-KDx27œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-KDx27{œfieldNameœ:œDocumentœ,œidœ:œPrompt-KDx27œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-KDx27",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-YXgUi",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-YXgUiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-YXgUi{œfieldNameœ:œDocumentœ,œidœ:œPrompt-YXgUiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-YXgUi",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-7lvWp",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-7lvWpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-7lvWp{œfieldNameœ:œDocumentœ,œidœ:œPrompt-7lvWpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-7lvWp",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-a7YLm",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-a7YLmœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-a7YLm{œfieldNameœ:œDocumentœ,œidœ:œPrompt-a7YLmœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-a7YLm",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-Hrgu7",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Hrgu7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-Hrgu7{œfieldNameœ:œDocumentœ,œidœ:œPrompt-Hrgu7œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-Hrgu7",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-uyXpe",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-uyXpeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-uyXpe{œfieldNameœ:œDocumentœ,œidœ:œPrompt-uyXpeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-uyXpe",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-DpcNf",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-DpcNfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-DpcNf{œfieldNameœ:œDocumentœ,œidœ:œPrompt-DpcNfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-DpcNf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-MsQev",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-MsQevœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-MsQev{œfieldNameœ:œDocumentœ,œidœ:œPrompt-MsQevœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-MsQev",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-IDdkJ",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-IDdkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-IDdkJ{œfieldNameœ:œDocumentœ,œidœ:œPrompt-IDdkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-IDdkJ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-mzw6H",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-mzw6Hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-mzw6H{œfieldNameœ:œDocumentœ,œidœ:œPrompt-mzw6Hœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-mzw6H",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-OwAmP",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-OwAmPœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-OwAmP{œfieldNameœ:œDocumentœ,œidœ:œPrompt-OwAmPœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-OwAmP",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "ParseData-O3xbr",
        "target": "Prompt-7xuDl",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œDocumentœ,œidœ:œPrompt-7xuDlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-ParseData-O3xbr{œdataTypeœ:œParseDataœ,œidœ:œParseData-O3xbrœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-7xuDl{œfieldNameœ:œDocumentœ,œidœ:œPrompt-7xuDlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-O3xbr",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "Document",
            "id": "Prompt-7xuDl",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-LEw60",
        "target": "GoogleGenerativeAIModel-VXx57",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-LEw60œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-LEw60{œdataTypeœ:œPromptœ,œidœ:œPrompt-LEw60œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-VXx57{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-LEw60",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-VXx57",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-0TzDg",
        "target": "GoogleGenerativeAIModel-Ai3sG",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-0TzDgœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-0TzDg{œdataTypeœ:œPromptœ,œidœ:œPrompt-0TzDgœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Ai3sG{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-0TzDg",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Ai3sG",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-QYsZH",
        "target": "GoogleGenerativeAIModel-vEuJf",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-QYsZHœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-QYsZH{œdataTypeœ:œPromptœ,œidœ:œPrompt-QYsZHœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-vEuJf{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-QYsZH",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-vEuJf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-YSKtu",
        "target": "GoogleGenerativeAIModel-AdnE6",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-YSKtuœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-YSKtu{œdataTypeœ:œPromptœ,œidœ:œPrompt-YSKtuœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-AdnE6{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-YSKtu",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-AdnE6",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-aYNdd",
        "target": "GoogleGenerativeAIModel-tyjTA",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-aYNddœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-aYNdd{œdataTypeœ:œPromptœ,œidœ:œPrompt-aYNddœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-tyjTA{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-aYNdd",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-tyjTA",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-yPJsv",
        "target": "GoogleGenerativeAIModel-dGFo2",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-yPJsvœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-yPJsv{œdataTypeœ:œPromptœ,œidœ:œPrompt-yPJsvœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-dGFo2{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-yPJsv",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-dGFo2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-QOkbs",
        "target": "GoogleGenerativeAIModel-EIUXq",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-QOkbsœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-QOkbs{œdataTypeœ:œPromptœ,œidœ:œPrompt-QOkbsœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-EIUXq{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-QOkbs",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-EIUXq",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-sufyN",
        "target": "CombineText-pJSlb",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-sufyNœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-pJSlbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-sufyN{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-sufyNœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-pJSlb{œfieldNameœ:œtext2œ,œidœ:œCombineText-pJSlbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-sufyN",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-pJSlb",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-pJSlb",
        "target": "GoogleGenerativeAIModel-itbVX",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-pJSlbœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-pJSlb{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-pJSlbœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-itbVX{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-pJSlb",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-itbVX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-p56dl",
        "target": "GoogleGenerativeAIModel-SL95I",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-p56dlœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-p56dl{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-p56dlœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-SL95I{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-p56dl",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-SL95I",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-DpcNf",
        "target": "GoogleGenerativeAIModel-SL95I",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-DpcNfœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-DpcNf{œdataTypeœ:œPromptœ,œidœ:œPrompt-DpcNfœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-SL95I{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-DpcNf",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-SL95I",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-9R3Fe",
        "target": "CombineText-p56dl",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-9R3Feœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-p56dlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-9R3Fe{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-9R3Feœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-p56dl{œfieldNameœ:œtext2œ,œidœ:œCombineText-p56dlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-9R3Fe",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-p56dl",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-pseV4",
        "target": "GoogleGenerativeAIModel-ZfQia",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-pseV4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-pseV4{œdataTypeœ:œPromptœ,œidœ:œPrompt-pseV4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ZfQia{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-pseV4",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-ZfQia",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-Vuct4",
        "target": "GoogleGenerativeAIModel-jtUke",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-Vuct4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-Vuct4{œdataTypeœ:œPromptœ,œidœ:œPrompt-Vuct4œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-jtUke{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-Vuct4",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-jtUke",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-vOZgf",
        "target": "CombineText-HtGPx",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-vOZgfœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-HtGPxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-vOZgf{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-vOZgfœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-HtGPx{œfieldNameœ:œtext2œ,œidœ:œCombineText-HtGPxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-vOZgf",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-HtGPx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-MsQev",
        "target": "GoogleGenerativeAIModel-F9HPH",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-MsQevœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-MsQev{œdataTypeœ:œPromptœ,œidœ:œPrompt-MsQevœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-F9HPH{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-MsQev",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-F9HPH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-HtGPx",
        "target": "GoogleGenerativeAIModel-F9HPH",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-HtGPxœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-HtGPx{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-HtGPxœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-F9HPH{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-HtGPx",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-F9HPH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-IDdkJ",
        "target": "GoogleGenerativeAIModel-JwW1V",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-IDdkJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-IDdkJ{œdataTypeœ:œPromptœ,œidœ:œPrompt-IDdkJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-JwW1V{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-IDdkJ",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-JwW1V",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-dWWyZ",
        "target": "GoogleGenerativeAIModel-JwW1V",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-dWWyZœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-dWWyZ{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-dWWyZœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-JwW1V{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-dWWyZ",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-JwW1V",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-pCf0i",
        "target": "CombineText-dWWyZ",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-pCf0iœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-dWWyZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-pCf0i{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-pCf0iœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-dWWyZ{œfieldNameœ:œtext2œ,œidœ:œCombineText-dWWyZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-pCf0i",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-dWWyZ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-zKzbb",
        "target": "GoogleGenerativeAIModel-1vipg",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-zKzbbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-zKzbb{œdataTypeœ:œPromptœ,œidœ:œPrompt-zKzbbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-1vipg{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-zKzbb",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-1vipg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-R5RgD",
        "target": "GoogleGenerativeAIModel-Tkx8P",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-R5RgDœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-R5RgD{œdataTypeœ:œPromptœ,œidœ:œPrompt-R5RgDœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Tkx8P{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-R5RgD",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Tkx8P",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-7yzWp",
        "target": "CombineText-FU0VT",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-7yzWpœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-FU0VTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-7yzWp{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-7yzWpœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-FU0VT{œfieldNameœ:œtext2œ,œidœ:œCombineText-FU0VTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-7yzWp",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-FU0VT",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-FU0VT",
        "target": "GoogleGenerativeAIModel-Uhy9L",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-FU0VTœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-FU0VT{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-FU0VTœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Uhy9L{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-FU0VT",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Uhy9L",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-mzw6H",
        "target": "GoogleGenerativeAIModel-Uhy9L",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-mzw6Hœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-mzw6H{œdataTypeœ:œPromptœ,œidœ:œPrompt-mzw6Hœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Uhy9L{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-mzw6H",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Uhy9L",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-7xuDl",
        "target": "GoogleGenerativeAIModel-evcyx",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-7xuDlœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-7xuDl{œdataTypeœ:œPromptœ,œidœ:œPrompt-7xuDlœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-evcyx{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-7xuDl",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-evcyx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-w9zLG",
        "target": "GoogleGenerativeAIModel-evcyx",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-w9zLGœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-w9zLG{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-w9zLGœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-evcyx{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-w9zLG",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-evcyx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-2zfLq",
        "target": "CombineText-w9zLG",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-2zfLqœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-w9zLGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-2zfLq{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-2zfLqœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-w9zLG{œfieldNameœ:œtext2œ,œidœ:œCombineText-w9zLGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-2zfLq",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-w9zLG",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-OwAmP",
        "target": "GoogleGenerativeAIModel-41cGM",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-OwAmPœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-OwAmP{œdataTypeœ:œPromptœ,œidœ:œPrompt-OwAmPœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-41cGM{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-OwAmP",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-41cGM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-Hrgu7",
        "target": "GoogleGenerativeAIModel-oTIYg",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-Hrgu7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-Hrgu7{œdataTypeœ:œPromptœ,œidœ:œPrompt-Hrgu7œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-oTIYg{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-Hrgu7",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-oTIYg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-2XThH",
        "target": "GoogleGenerativeAIModel-oTIYg",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-2XThHœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-2XThH{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-2XThHœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-oTIYg{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-2XThH",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-oTIYg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-zZa6D",
        "target": "CombineText-2XThH",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-zZa6Dœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-2XThHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-zZa6D{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-zZa6Dœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-2XThH{œfieldNameœ:œtext2œ,œidœ:œCombineText-2XThHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-zZa6D",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-2XThH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-kzRW8",
        "target": "CombineText-BONhX",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-kzRW8œ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-BONhXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-kzRW8{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-kzRW8œ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-BONhX{œfieldNameœ:œtext2œ,œidœ:œCombineText-BONhXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-kzRW8",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-BONhX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-BONhX",
        "target": "GoogleGenerativeAIModel-ry2Rb",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-BONhXœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-BONhX{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-BONhXœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ry2Rb{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-BONhX",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-ry2Rb",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-a7YLm",
        "target": "GoogleGenerativeAIModel-ry2Rb",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-a7YLmœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-a7YLm{œdataTypeœ:œPromptœ,œidœ:œPrompt-a7YLmœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ry2Rb{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-a7YLm",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-ry2Rb",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-TZNTF",
        "target": "CombineText-skF0K",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-TZNTFœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-skF0Kœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-TZNTF{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-TZNTFœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-skF0K{œfieldNameœ:œtext2œ,œidœ:œCombineText-skF0Kœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-TZNTF",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-skF0K",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-skF0K",
        "target": "GoogleGenerativeAIModel-X88mX",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-skF0Kœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-skF0K{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-skF0Kœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-X88mX{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-skF0K",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-X88mX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-7lvWp",
        "target": "GoogleGenerativeAIModel-X88mX",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-7lvWpœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-7lvWp{œdataTypeœ:œPromptœ,œidœ:œPrompt-7lvWpœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-X88mX{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-7lvWp",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-X88mX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-s1NNy",
        "target": "CombineText-QVHkJ",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-s1NNyœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-QVHkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-s1NNy{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-s1NNyœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-QVHkJ{œfieldNameœ:œtext2œ,œidœ:œCombineText-QVHkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-s1NNy",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-QVHkJ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-YXgUi",
        "target": "GoogleGenerativeAIModel-HTcbf",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-YXgUiœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-YXgUi{œdataTypeœ:œPromptœ,œidœ:œPrompt-YXgUiœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-HTcbf{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-YXgUi",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-HTcbf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-QVHkJ",
        "target": "GoogleGenerativeAIModel-HTcbf",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-QVHkJœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-QVHkJ{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-QVHkJœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-HTcbf{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-QVHkJ",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-HTcbf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-KDx27",
        "target": "GoogleGenerativeAIModel-Qa1Vw",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-KDx27œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-KDx27{œdataTypeœ:œPromptœ,œidœ:œPrompt-KDx27œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Qa1Vw{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-KDx27",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Qa1Vw",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-438mj",
        "target": "GoogleGenerativeAIModel-Qa1Vw",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-438mjœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-438mj{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-438mjœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Qa1Vw{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-438mj",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Qa1Vw",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-FrgZ2",
        "target": "CombineText-438mj",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-FrgZ2œ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-438mjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-FrgZ2{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-FrgZ2œ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-438mj{œfieldNameœ:œtext2œ,œidœ:œCombineText-438mjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-FrgZ2",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-438mj",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-IieDF",
        "target": "CombineText-1leNx",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-IieDFœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-1leNxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-IieDF{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-IieDFœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-1leNx{œfieldNameœ:œtext2œ,œidœ:œCombineText-1leNxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-IieDF",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-1leNx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-1leNx",
        "target": "GoogleGenerativeAIModel-7qjJi",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-1leNxœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-1leNx{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-1leNxœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-7qjJi{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-1leNx",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-7qjJi",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-Bia8u",
        "target": "GoogleGenerativeAIModel-7qjJi",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-Bia8uœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-Bia8u{œdataTypeœ:œPromptœ,œidœ:œPrompt-Bia8uœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-7qjJi{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-Bia8u",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-7qjJi",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GlassCompositionConverter-WHWvv",
        "target": "CombineText-bA6y9",
        "sourceHandle": "{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-WHWvvœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-bA6y9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GlassCompositionConverter-WHWvv{œdataTypeœ:œGlassCompositionConverterœ,œidœ:œGlassCompositionConverter-WHWvvœ,œnameœ:œconverted_dataœ,œoutput_typesœ:[œMessageœ]}-CombineText-bA6y9{œfieldNameœ:œtext2œ,œidœ:œCombineText-bA6y9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GlassCompositionConverter",
            "id": "GlassCompositionConverter-WHWvv",
            "name": "converted_data",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-bA6y9",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-bA6y9",
        "target": "GoogleGenerativeAIModel-ppBan",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-bA6y9œ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-bA6y9{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-bA6y9œ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ppBan{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-bA6y9",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-ppBan",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-po5vR",
        "target": "GoogleGenerativeAIModel-ppBan",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-po5vRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-po5vR{œdataTypeœ:œPromptœ,œidœ:œPrompt-po5vRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ppBan{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-po5vR",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-ppBan",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-VOPkO",
        "target": "GoogleGenerativeAIModel-NTWiN",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-VOPkOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-VOPkO{œdataTypeœ:œPromptœ,œidœ:œPrompt-VOPkOœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-NTWiN{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-VOPkO",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-NTWiN",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-BTfkR",
        "target": "GoogleGenerativeAIModel-Y5uD2",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-BTfkRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-BTfkR{œdataTypeœ:œPromptœ,œidœ:œPrompt-BTfkRœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Y5uD2{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-BTfkR",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-Y5uD2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-95tm9",
        "target": "GoogleGenerativeAIModel-fa2Vp",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-95tm9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-Prompt-95tm9{œdataTypeœ:œPromptœ,œidœ:œPrompt-95tm9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-fa2Vp{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-95tm9",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-fa2Vp",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "CombineText-JV5OM",
        "target": "EnvoyerDonneesVerreTableComponent-D5zFJ",
        "sourceHandle": "{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-JV5OMœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtexte_extraitœ,œidœ:œEnvoyerDonneesVerreTableComponent-D5zFJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-CombineText-JV5OM{œdataTypeœ:œCombineTextœ,œidœ:œCombineText-JV5OMœ,œnameœ:œcombined_textœ,œoutput_typesœ:[œMessageœ]}-EnvoyerDonneesVerreTableComponent-D5zFJ{œfieldNameœ:œtexte_extraitœ,œidœ:œEnvoyerDonneesVerreTableComponent-D5zFJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "CombineText",
            "id": "CombineText-JV5OM",
            "name": "combined_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "texte_extrait",
            "id": "EnvoyerDonneesVerreTableComponent-D5zFJ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-7qjJi",
        "target": "CombineText-JV5OM",
        "sourceHandle": "{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "targetHandle": "{œfieldNameœ:œtext5œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "id": "reactflow__edge-GoogleGenerativeAIModel-7qjJi{œdataTypeœ:œGoogleGenerativeAIModelœ,œidœ:œGoogleGenerativeAIModel-7qjJiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext5œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "sourceHandle": {
            "dataType": "GoogleGenerativeAIModel",
            "id": "GoogleGenerativeAIModel-7qjJi",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "text5",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "File-2qZjd",
        "target": "ParseData-O3xbr",
        "sourceHandle": "{œdataTypeœ:œFileœ,œidœ:œFile-2qZjdœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}",
        "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-O3xbrœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "id": "reactflow__edge-File-2qZjd{œdataTypeœ:œFileœ,œidœ:œFile-2qZjdœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-ParseData-O3xbr{œfieldNameœ:œdataœ,œidœ:œParseData-O3xbrœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "data": {
          "sourceHandle": {
            "dataType": "File",
            "id": "File-2qZjd",
            "name": "data",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "data",
            "id": "ParseData-O3xbr",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "selected": false,
        "className": "",
        "animated": false
      },
      {
        "source": "Prompt-uyXpe",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-uyXpeœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-itbVX",
        "targetHandle": "{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "system_message",
            "id": "GoogleGenerativeAIModel-itbVX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-uyXpe",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__Prompt-uyXpe{œdataTypeœ:œPromptœ,œidœ:œPrompt-uyXpeœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-itbVX{œfieldNameœ:œsystem_messageœ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": ""
      },
      {
        "source": "GoogleGenerativeAIModel-Y5uD2",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-nDAxX",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-nDAxXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-nDAxX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-Y5uD2",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-Y5uD2{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-nDAxX{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-nDAxXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-H9Xox",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-Sblyw",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-Sblywœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-Sblyw",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-H9Xox",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-H9Xox{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-Sblyw{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-Sblywœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-NTWiN",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-WHWvv",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-WHWvvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-WHWvv",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-NTWiN",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-NTWiN{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-WHWvv{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-WHWvvœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-VXx57",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-IieDF",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-IieDFœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-IieDF",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-VXx57",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-VXx57{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-IieDF{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-IieDFœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-Ai3sG",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-FrgZ2",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-FrgZ2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-FrgZ2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-Ai3sG",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-Ai3sG{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-FrgZ2{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-FrgZ2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-vEuJf",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-s1NNy",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-s1NNyœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-s1NNy",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-vEuJf",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-vEuJf{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-s1NNy{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-s1NNyœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-AdnE6",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-TZNTF",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-TZNTFœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-TZNTF",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-AdnE6",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-AdnE6{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-TZNTF{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-TZNTFœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-tyjTA",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-kzRW8",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-kzRW8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-kzRW8",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-tyjTA",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-tyjTA{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-kzRW8{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-kzRW8œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-dGFo2",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-zZa6D",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-zZa6Dœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-zZa6D",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-dGFo2",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-dGFo2{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-zZa6D{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-zZa6Dœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-EIUXq",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-sufyN",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-sufyNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-sufyN",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-EIUXq",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-EIUXq{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-sufyN{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-sufyNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-ZfQia",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-9R3Fe",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-9R3Feœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-9R3Fe",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-ZfQia",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-ZfQia{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-9R3Fe{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-9R3Feœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-jtUke",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-vOZgf",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-vOZgfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-vOZgf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-jtUke",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-jtUke{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-vOZgf{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-vOZgfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-1vipg",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-pCf0i",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-pCf0iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-pCf0i",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-1vipg",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-1vipg{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-pCf0i{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-pCf0iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-Tkx8P",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-7yzWp",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-7yzWpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-7yzWp",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-Tkx8P",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-Tkx8P{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-7yzWp{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-7yzWpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-41cGM",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GlassCompositionConverter-2zfLq",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-2zfLqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_text",
            "id": "GlassCompositionConverter-2zfLq",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-41cGM",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-41cGM{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GlassCompositionConverter-2zfLq{œfieldNameœ:œinput_textœ,œidœ:œGlassCompositionConverter-2zfLqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-fa2Vp",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-fa2Vp",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-fa2Vp{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext1œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-FfeYQ",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext2œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text2",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-FfeYQ",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-FfeYQ{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-FfeYQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext2œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-yDfoE",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext3œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text3",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-yDfoE",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-yDfoE{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-yDfoEœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext3œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-ppBan",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext4œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text4",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-ppBan",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-ppBan{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ppBanœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext4œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-Qa1Vw",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext6œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text6",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-Qa1Vw",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-Qa1Vw{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Qa1Vwœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext6œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-HTcbf",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext7œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text7",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-HTcbf",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-HTcbf{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-HTcbfœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext7œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-X88mX",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext8œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text8",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-X88mX",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-X88mX{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-X88mXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext8œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-ry2Rb",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext9œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text9",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-ry2Rb",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-ry2Rb{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-ry2Rbœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext9œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-oTIYg",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext10œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text10",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-oTIYg",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-oTIYg{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-oTIYgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext10œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-itbVX",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext11œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text11",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-itbVX",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-itbVX{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-itbVXœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext11œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-SL95I",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext12œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text12",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-SL95I",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-SL95I{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-SL95Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext12œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-F9HPH",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext13œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text13",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-F9HPH",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-F9HPH{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-F9HPHœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext13œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-JwW1V",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext14œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text14",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-JwW1V",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-JwW1V{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-JwW1Vœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext14œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-Uhy9L",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext15œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text15",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-Uhy9L",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-Uhy9L{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-Uhy9Lœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext15œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-evcyx",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-JV5OM",
        "targetHandle": "{œfieldNameœ:œtext16œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text16",
            "id": "CombineText-JV5OM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-evcyx",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-evcyx{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-evcyxœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-JV5OM{œfieldNameœ:œtext16œ,œidœ:œCombineText-JV5OMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-fa2Vp",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-fa2Vp",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-fa2Vp{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-fa2Vpœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-VXx57",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-VXx57",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-VXx57{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-VXx57œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-Ai3sG",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Ai3sG",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Ai3sG{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Ai3sGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-vEuJf",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-vEuJf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-vEuJf{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-vEuJfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-AdnE6",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-AdnE6",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-AdnE6{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-AdnE6œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-tyjTA",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-tyjTA",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-tyjTA{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-tyjTAœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-dGFo2",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-dGFo2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-dGFo2{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-dGFo2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-EIUXq",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-EIUXq",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-EIUXq{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-EIUXqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-ZfQia",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-ZfQia",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-ZfQia{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-ZfQiaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-jtUke",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-jtUke",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-jtUke{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-jtUkeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-1vipg",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-1vipg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-1vipg{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-1vipgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-Tkx8P",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Tkx8P",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Tkx8P{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Tkx8Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-41cGM",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-41cGM",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-41cGM{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-41cGMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-NTWiN",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-NTWiN",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-NTWiN{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-NTWiNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-H9Xox",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-H9Xox",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-H9Xox{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-H9Xoxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "GoogleGenerativeAIModel-Y5uD2",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "GoogleGenerativeAIModel-Y5uD2",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-GoogleGenerativeAIModel-Y5uD2{œfieldNameœ:œinput_valueœ,œidœ:œGoogleGenerativeAIModel-Y5uD2œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-2gWvC",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-2gWvCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-2gWvC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-2gWvC{œfieldNameœ:œtext1œ,œidœ:œCombineText-2gWvCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-DHXvR",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-DHXvRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-DHXvR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-DHXvR{œfieldNameœ:œtext1œ,œidœ:œCombineText-DHXvRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-bA6y9",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-bA6y9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-bA6y9",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-bA6y9{œfieldNameœ:œtext1œ,œidœ:œCombineText-bA6y9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-1leNx",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-1leNxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-1leNx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-1leNx{œfieldNameœ:œtext1œ,œidœ:œCombineText-1leNxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-438mj",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-438mjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-438mj",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-438mj{œfieldNameœ:œtext1œ,œidœ:œCombineText-438mjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-QVHkJ",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-QVHkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-QVHkJ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-QVHkJ{œfieldNameœ:œtext1œ,œidœ:œCombineText-QVHkJœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-skF0K",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-skF0Kœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-skF0K",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-skF0K{œfieldNameœ:œtext1œ,œidœ:œCombineText-skF0Kœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-BONhX",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-BONhXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-BONhX",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-BONhX{œfieldNameœ:œtext1œ,œidœ:œCombineText-BONhXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-2XThH",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-2XThHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-2XThH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-2XThH{œfieldNameœ:œtext1œ,œidœ:œCombineText-2XThHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-pJSlb",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-pJSlbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-pJSlb",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-pJSlb{œfieldNameœ:œtext1œ,œidœ:œCombineText-pJSlbœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-p56dl",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-p56dlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-p56dl",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-p56dl{œfieldNameœ:œtext1œ,œidœ:œCombineText-p56dlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-HtGPx",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-HtGPxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-HtGPx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-HtGPx{œfieldNameœ:œtext1œ,œidœ:œCombineText-HtGPxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-dWWyZ",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-dWWyZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-dWWyZ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-dWWyZ{œfieldNameœ:œtext1œ,œidœ:œCombineText-dWWyZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-FU0VT",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-FU0VTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-FU0VT",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-FU0VT{œfieldNameœ:œtext1œ,œidœ:œCombineText-FU0VTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "GoogleGenerativeAIModel-uYxja",
        "sourceHandle": "{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CombineText-w9zLG",
        "targetHandle": "{œfieldNameœ:œtext1œ,œidœ:œCombineText-w9zLGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text1",
            "id": "CombineText-w9zLG",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "LiteLLM IXIA V0.3",
            "id": "GoogleGenerativeAIModel-uYxja",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "xy-edge__GoogleGenerativeAIModel-uYxja{œdataTypeœ:œLiteLLM IXIA V0.3œ,œidœ:œGoogleGenerativeAIModel-uYxjaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CombineText-w9zLG{œfieldNameœ:œtext1œ,œidœ:œCombineText-w9zLGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      }
    ],
    "viewport": {
      "x": -160.94597607492983,
      "y": -363.090289434405,
      "zoom": 0.3203350637484659
    }
  },
  "description": "Transform Your Business with Smart Dialogues.",
  "name": "LANGFLOW_FLASK_GLASS",
  "last_tested_version": "1.2.0",
  "endpoint_name": null,
  "is_component": false
}